%TODO: Correct the Gflops and results
%TODO: Check tensor methods
\begin{abstract}
%
The tensor-matrix multiplication is a basic tensor operation required by various tensor methods such as the ALS and the HOSVD.
%
This paper presents flexible high-performance algorithms that compute the tensor-matrix product according to the Loops-over-GEMM (LoG) approach.
%
Our algorithms can process dense tensors with any linear tensor layout, arbitrary tensor order and dimensions all of which can be runtime variable.
%
We discuss different tensor slicing methods with parallelization strategies and propose six algorithm versions which call \tf{gemv}, \tf{gemm} and/or \tf{gemm\_batch} routines with subtensors or tensor slices.
%
Their performance is quantified on a set of tensors with various shapes and tensor orders.
%
Our best performing version attains a median performance of $1.37$ double precision Tflops on an Intel Xeon Gold 6248R processor using Intel's MKL.
%
We show that the performance is only slightly affected by the tensor layout.
% and the median performance is between \tq and \tq Tflops for a range of linear tensor formats.
%todo: correct the sentence.
%
Our fastest implementation is on average at least $14.05$\% and up to $3.79$ x faster than other state-of-the-art approaches and actively developed libraries like Libtorch and Eigen.
%todo: check if that is true.
%\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
