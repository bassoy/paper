%TODO: Correct the Gflops and results
%TODO: Check tensor methods
\begin{abstract}
%
The tensor-matrix product is a basic tensor operation that is required by various tensor methods such as the ALS or the HOSVD.
%
This paper presents flexible high-performance algorithms for the mode-$q$ tensor-matrix multiplication that compute the product according to the Loops-over-\tf{gemm}s (LOG) approach.
%
Our algorithms can process dense tensors with any linear tensor layout, arbitrary tensor order and dimensions all of which can be runtime variable.
%
We discuss different tensor slicing methods with parallelization strategies and propose six versions which call \tf{gemv}, \tf{gemm} and/or \tf{gemm\_batch} with subtensors or tensor slices.
%
Their performance is quantified for a set of tensors with various shapes and tensor order.
%
Our best performing version attains a median performance of $1.37$ double precision Tflops on an Intel Xeon Gold 6248R processor using Intel's MKL.
%
We show that the performance is only slightly affected by the tensor layout and the median performance is between \tq and \tq Tflops for a range of linear tensor formats.
%
Our fastest implementation is on average at least by $14.05$\% and up to $3.79$ x faster than other state-of-the-art implementations, including Libtorch and Eigen.
%todo: check if that is true.
%\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
