\section{Background}
\label{sec:preliminaries}

\subsubsection{Notation}
\label{sec:preliminaries:notation}
An order-$p$ tensor is a $p$-dimensional array \cite{lim:2017:hypermatrices} where tensor elements are contiguously stored in memory. % \cite{lee:2018:fundamental}
We write $a$, $\mba$, $\mbA$ and $\mubA$ in order to denote scalars, vectors, matrices and tensors. 
In general we assume a tensor $\mubA$ to have a tensor order with $p>2$.
The $p$-tuple $\mbn$ with $\mbn = (n_1,n_2,\dots,n_p)$ will be referred to as a dimension tuple with $n_r>1$.
We will use round brackets $\mubA(i_1,i_2,\dots,i_p)$ or $\mubA(\mbi)$ to denote a tensor element where $\mbi = (i_1,i_2,\dots,i_p)$ is a multi-index.
%The set of all multi-indices of a tensor is denoted by $\mcI \in \bbN^p$ with $\mcI = \prod{r=1}^p I_r$ and $I_r = \{1,\dots,n_r\}$.
%The set $\mcJ = \{0,\dots,\bar{n}-1\}$ shall denote the single-index space of a tensor with $\bar{n} = n_1 \cdot n_2 \cdots n_p$ contiguously stored elements where $|\mcI| = |\mcJ|$.
%TODO: do we still need subtensor definition?

A subtensor denoted by $\mubA'$ references a subset of tensor elements.
The subtensor elements are specified with $p$ index ranges and form a selection grid.
The $r$-th index range shall be given by an index pair denoted by $f_r \colon l_r$ with $1 \leq f_r \leq l_r \leq n_r$ with $l_r - f_r + 1 = n_r'$.
%TODO: correct the $p'$
A subtensor is called a slice $\mubA_{u,v}'$ if two modes $1 \leq u \neq v \leq p$ of the corresponding tensor $\mubA$ are selected with a full index range.
The remaining modes are selected with a single index so that only two dimensions of the slice are greater than one.
A fiber $\mubA_u'$ is a tensor slice with only one dimension greater than $1$.


\subsubsection{Linear Tensor Layouts}
\label{sec:preliminaries:layout}
We use a layout tuple $\mbpi \in \bbN^p$ to encode all linear tensor layouts including the first-order or last-order layout.
They contain permuted tensor modes whose priority is given by their index.
For instance, the first- and last-order storage formats are given by $\mbpi_F = (1,2,\dots,p)$ and $\mbpi_{L} = (p,p-1,\dots,1)$.
An inverse layout tuple $\mbpi^{-1}$ is defined by $\mbpi^{-1}(\mbpi(k)) = k$.
Given a layout tuple $\mbpi$ with $p$ modes, the $\pi_r$-th element of a stride tuple is given by $w_{\pi_r} = \prod_{k=1}^{r-1} n_{\pi_k}$ for $1 < r \leq p$ and $w_{\pi_1} = 1$.
Tensor elements of the $\pi_1$-th mode are contiguously stored in memory.

The location of tensor elements within the allocated memory space is determined by the tensor layout and the corresponding layout function.
For a given layout and stride tuple, a layout function $\lambda_{\mbw}$ maps a multi-index to a scalar index with $\lambda_{\mbw}(\mbi) = \sum_{r=1}^p w_r (i_r-1)$.
With $j = \lambda_{\mbw} (\mbi)$ being the relative memory position of an element with a multi-index $\mbi$, reading from and writing to memory is accomplished with $j$ and the first element's address of $\mubA$.


\subsubsection{Non-Modifying Flattening and Reshaping}
\label{sec:preliminaries:flattening.reshaping}
The flattening operation $\varphi_{r,q}$ transforms an order-$p$ tensor $\mubA$ to another order-$p'$ view $\mubB$ that has different a shape $\mbm$ and layout $\mbtau$ tuple of length $p'$ with $p' = p-q+r$ and $1 \leq r < q \leq p$.
It is related to the tensor unfolding operation as defined in \cite[p.459]{kolda:2009:decompositions} but neither changes the element ordering nor copies tensor elements.
Given a layout tuple $\mbpi$ of $\mubA$, the flattening operation $\varphi_{r,q}$ is defined for contiguous modes $\mbhpi = (\pi_r,\pi_{r+1}, \dots, \pi_{q})$ of $\mbpi$.
Let $j = 0$ if $k \leq r$ and $j = q-r$ otherwise for $1 \leq k \leq p'$.
%Let also $s_k = 0$ if $\pi_k < \max(\mbpi_{r,q})$ and $s_k = q-r$ if $\pi_{k+j} > \max(\mbpi_{r,q})$.
Then the resulting layout tuple $\mbtau = (\tau_1,\dots,\tau_{p'})$ of $\mubB$ is given by $\tau_r = \min(\mbpi_{r,q})$ and $\tau_{k} = \pi_{k+j} + s_k$ if $k \neq r$ where $s_k = \left| \{ \pi_i \mid \pi_{k+j} > \pi_i \wedge \pi_i \neq \min(\mbhpi) \wedge r \leq i \leq p \} \right|$.
Elements of the corresponding shape tuple $\mbm$ are given by $m_{\tau_r} = \prod_{k=r}^q n_{\pi_k}$ and $m_{\tau_k} = n_{\pi_{k+j}}$ if $k \neq r$.

The reshaping operation $\rho$ transforms an order-$p$ tensor $\mubA$ to another order-$p$ tensor $\mubB$ with different shape $\mbm$ and layout $\mbtau$ tuples of length $p$.
In this work, it permutes the shape and layout tuple simultaneously without changing the element ordering and without copying tensor elements.
The operation $\rho$ uses a permutation tuple $\mbrho = (\rho_1,\dots,\rho_p)$ to only modify shape and layout tuples.
Elements of the resulting shape tuple $\mbm$ and the layout tuple $\mbtau$ are given by $m_r = n_{\rho_r}$ and $\tau_r = \pi_{\rho_r}$, respectively.



%TODO: use other frameworks to explain how those two index sets are connected.
%The location of tensor elements within the allocated memory space is determined by the storage format of a tensor and the corresponding \tit{layout} \tit{function}.
%For a given layout and stride tuple, a layout function $\lambda_{\mbw} : \mcI \rightarrow \mcJ$ maps a multi-index to a scalar index according to 
%\begin{equation}
%\label{equ:lambda}
%\lambda_{\mbw}(\mbi) = \sum_{r=1}^p w_r (i_r-1)
%\end{equation}
%With $j = \lambda_{\mbw} (\mbi)$ being the relative memory position of an element with a multi-index $\mbi$, reading from and writing to memory is accomplished with $j$ and the first element's address of $\mubA$.

%
%from \cite{bassoy:2018:fast,rogers:2016:efficient}
%\end{equation}
%the following mappings include any non-hierarchical layouts that can be specified by a layout tuple $\mbpi$.\todo{Fertigstellen.} 
%The layout function $\lambda_{\mbw}$ with 
%The inverse layout function $\lambda_{\mbw}^{-1}  : \mcJ \rightarrow \mcI$ of $\lambda_{\mbw}$ with $\mbi = \lambda_{\mbw}^{-1}(j)$ transforms scalar indices with\todo{This can go if the only shifting is sufficient.}
%\begin{equation}
%\label{equ:lambda_inv}
%i_{r} = \left\lfloor \frac{k_{r}}{w_{r}} \right\rfloor+1 
%\quad \text{with} \quad 
%k_{\pi_r} = k_{\pi_{r+1}} - w_{\pi_{r+1}} \cdot i_{\pi_{r+1}} \ \text{for} \ r < p
%\end{equation}
%and $i_{\pi_p} =  \lfloor j/ w_{\pi_p} \rfloor+1$. 


%Both layout functions are also valid for transforming multi-index and scalar indices for a subtensor.
%With  $k_r \in K_r$ we will use round brackets again $\mubA'(k_1,\dots,k_p)$ or $\mubA'(\mbk)$ to identify subtensor elements.
%The correspondence between tensor and subtensor elements for the $r$-th mode is given by the linear function $\gamma_r(k_r) = f_r + (k_r-1) \cdot t_r$ such that $\mubS(k_1,\dots,k_p) = \mubA(\gamma_1(k_1),\dots,\gamma_p(k_p))$.
%Subtensor elements can be accessed with single indices by applying the function $\lambda_{\mbw} \circ \gamma \circ \gamma_{\mbv}$ where $\mbv$ and $\mbw$ are stride tuples of a subtensor and tensor. 

%We can analogously define a scalar index set $\mcJ'$ for a subtensor with $\bar{n}'$ elements where $\bar{n}' = \prod_{r=1}^{p} n_r'$.
%Note that $\lambda$ can only be applied if $1 = f_r$, $l_r = n_r$ and $1 = t_r$ such that $n_r' = n_r$. 
%The layout function $\lambda$ however cannot be directly applied if any index triplet $(f_r,t_r,l_r)$ satisfies $1 < f_r$, $l_r < n_r$ or $1 < t_r$ such that $n_r' < n_r$. 
%such that $j = \lambda_{\mbw} \left( \gamma \left ( \lambda_{\mbw'}^{-1} \left (j' \right ) \right ) \right )$ 


\subsubsection{Tensor-Matrix Multiplication (TTM)}
Let $\mubA$ and $\mubC$ be order-$p$ tensors with shapes $\mbn_a = (n_1,\dots,n_q,\dots,n_p)$ and $\mbn_c =(n_1,\dots,n_{q-1},m,n_{q+1},\dots,n_p)$. 
Let $\mbB$ be a matrix of shape $\mbn_b = (m,n_q)$.
A mode-$q$ TTM is denoted by $\mubC = \mubA \times_q \mbB$ where an element of $\mubC$ is given by
\begin{equation}
\label{equ:tensor.matrix.multplication}
\mubC(i_1, \dots, i_{q-1}, j, i_{q+1}, \dots, i_p) = \sum_{i_q=1}^{n_q} \mubA(i_1, \dots, i_q, \dots, i_p) \cdot \mbB(j,i_q)
\end{equation}
with $1 \leq i_r \leq n_r$ and $1 \leq j \leq m$.
%Similar to the tensor-vector multiplication, the multiplication consists of multiple inner productd of a fiber of $\mubA$ and $\mbb$.% with $1 \leq i_r \leq n_r$ and $1 \leq r \leq p$.
The mode $q$ is the \tit{contraction} \tit{mode} of the TTM  with $1 \leq q \leq p$.
The tensor-matrix multiplication generalizes the computational aspect of the two-dimensional case $\mbC = \mbB \cdot \mbA$ if $p=2$ and $q=1$.
Its arithmetic intensity is equal to that of a matrix-matrix multiplication and is not memory-bound.
%Categorized in \cite{dinapoli:2014:towards.efficient.use} as an operation of the tensor contraction class 2, its computation is likely to be limited by the memory bandwidth.
In the following, we assume that the tensors $\mubA$ and $\mubC$ have the same tensor layout $\mbpi$. 
Elements of matrix $\mubB$ can stored in either the column-major or row-major format.

