\section{Algorithm Design}
\label{sec:design}
\subsection{Sequential Baseline Algorithm}
\label{sec:design:sequential.baseline.algorithm}
The sequential baseline algorithm implementing Eq. \ref{equ:tensor.matrix.multplication} can be implemented with a single C++ function.
It consists of nested recursion with a control flow that resembles algorithm 1 in \cite{bassoy:2018:fast}, consisting of two \ttt{if} statements with an \ttt{else} branch.
The body of the first \ttt{if} statement contains a recursive call that skips the iteration over the dimension $n_{q}$ when $r = \mhq$ with $\pi_r = q$ and $\mhq = \mbpi^{-1}_q$ where $\mbpi^{-1}$ is the inverse layout tuple.
%todo: check if q or \mhq
% with 
The second \ttt{if} statement contains multiple recursive calls for the modes $1 \leq r \neq \mhq \leq p$ with different multi-indices.
Note that the second \ttt{if} statement is skipped for $q = \pi_1$ as the condition of the first one is evaluated to true.
The \ttt{else} branch is the base case and consists of two loops that compute a fiber-matrix product.
The inner loop iterates over the dimension $n_q$ of $\mubA$ and $\mbB$ with index $1\leq i_q \leq n_q$ computing an inner product.
The outer loop iterates over the dimension $m$ of $\mubC$ and $\mbB$ with index $1 \leq j \leq m$.
The baseline algorithm supports tensors with arbitrary order, dimensions and any non-hierarchical storage format.

\subsection{Modified Baseline Algorithm with Contiguous Memory Access}
\label{sec:design:modified.baseline.algorithm}
The baseline algorithm accesses memory of $\mubA$ and $\mubC$ non-contiguously whenever $\pi_1 \neq q$ so that indices $i_q$ and $j$ are incremented with steps greater than one.
Matrix $\mbB$ is contiguously accessed if $i_q$ or $j$ is incremented with unit-steps depending on the storage format of $\mubB$.
The access pattern could be improved by reordering tensor elements according to the storage format which results in copy operations reducing the overall throughput of the operation \cite{shi:2016:tensor.contraction}.

A better approach is to access tensor elements according to the tensor layout using the permutation tuple $\mbpi$ as proposed in \cite{bassoy:2018:fast}.
The modified algorithm with contiguous memory accesses is given in algorithm \ref{alg:ttm.sequential.coalesced} for $\pi_1 \neq q$ and $p > 1$.
Each recursion level adjusts only one multi-index element $i_{\pi_r}$ with a stride $w_{\pi_r}$ as depicted in line 5.
With increasing recursion level and decreasing $r$, indices are incremented with smaller step sizes as $w_{\pi_r} \leq w_{\pi_{r+1}}$. 
The condition of the second \ttt{if} statement in line 4 is changed from $r \geq 1$ to $r > 1$.
In this way, the loop incrementing with index $i_{\pi_1}$ and the minimum stride $w_{\pi_1}$ can be included in the base case which contains three loops performing a slice-matrix multiplication. 
The ordering of the three loops within the base case are adjusted according to the tensor and matrix layout.
The inner-most loop increments $i_{\pi_1}$ and therefore contiguously accesses tensor elements of $\mubA$ and $\mubC$.
The second loop increments $i_q$ with which elements of $\mbB$ are contiguously accessed if $\mbB$ is stored in the row-major format.
The third loop increments $j$ and could be placed as the second loop if $\mbB$ is stored in the column-major format.
The simple ordering of the three loops is discussed in \cite{golub:2013:matrix.computations}.

\begin{algorithm}[t]
%\SetAlgoNoLine
\DontPrintSemicolon
\SetKwProg{Fn}{}{}{end}
\SetKwFunction{function}{tensor\_times\_matrix}%
%\SetAlgoNoEnd
\footnotesize 
\SetAlgoVlined
\hrule
\BlankLine
\Fn{\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r$}}
{
	\uIf{$r = \mhq$ }
	{
		\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$ }
	}
	\uElseIf{$r > 1$ }
	{
		\For{$i_{\pi_r} \leftarrow 1$ \KwTo $ n_{\pi_r}$}
		{
			\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$}\;
		}		
	}	
	\Else%{$r\geq1 \wedge m \neq 1$}
	{
		\For{$j \leftarrow 1$ \KwTo $m$}
		{
			\For{$i_q \leftarrow 1$ \KwTo $n_q$}
			{			
				\For{$i_{\pi_1} \leftarrow 1$ \KwTo $n_{\pi_1}$}
				{
					$\mubC(i_1,...,i_{q-1},j,i_{q+1},...,i_{p})$ \ttt{+=} $\mubA(i_1,...,i_q,...,i_p) \cdot \mbB(j,i_q)$\;
				}
			}
		}
	}
}
\BlankLine
\hrule
\caption{
\footnotesize %
Modified baseline algorithm with contiguous memory access for the tensor-matrix multiplication.
The tensor order must be greater than one and for the contraction mode $1 \leq q \leq p$ and $\pi_1 \neq q$ must hold.
The algorithm needs to be initially called with $r=p$ where $\mbn$ is the shape tuple of $\mubA$ and $m$ is the $q$-th dimension of $\mubC$. 
%Iteration along mode $q$ with $\mhq = \mbpi^{-1}_q$ is moved into the inner-most recursion level.
\label{alg:ttm.sequential.coalesced}
}
\end{algorithm}

While spatial data locality is improved by adjusting the loop ordering, the temporal data locality of tensors $\mubA$ and $\mubC$ differ.
Note that slice $\mubA_{\pi_1,q}'$ is accessed $m$ times, fiber $\mubC_{\pi_1}$ is accessed $\mbn(q)$ times and element $\mubB(j,i_q)$ is accessed $\mbn(\pi_1)$ times.
While the specified fiber of $\mubC$ can fit into first or second level cache, slice elements of $\mubA$ are unlikely to fit in the local caches if the slice size $n_{\pi_1} \times n_q$ is large leading to higher cache misses and suboptimal performance.
%todo: hier mehr paper, die BLAS Implementierung referenzieren?
Optimized tiling for better temporal data locality has been discussed in \cite{goto:2008:gemm} which suggests to use existing high-performance BLAS implementations for the base case.
%The proposed algorithm therefore constitutes the starting point for \tf{BLAS} utilization within the base case.

\subsection{BLAS-based Algorithms with Tensor Slices}
\label{sec:design:blas.based.algorithm}
%\vspace{-0.3em}
The proposed algorithm \ref{alg:ttm.sequential.coalesced} is the starting point for the BLAS-based algorithm which computes the tensor-matrix product with a \tf{gemm} routine.
Besides the illustrated algorithm, we have identified seven other cases where a single \tf{gemm} call suffices to compute the tensor-matrix product even if the tensor order $p$ is greater than two.
In summary there are eight cases with a single \tf{gemm} call using different arguments which are listed in table \ref{tab:mapping}.
The list of \tf{gemm} calls is complete with no limitation on tensor order and contraction mode, supporting all linear tensor layout.
\tf{gemm} arguments are chosen depending on the tensor order $p$, tensor layout $\mbpi$ and contraction mode $q$ except for the \tf{CBLAS\_ORDER} which is \tf{CblasRowMajor}.
%Note , all linear tensor layouts can be supported by setting the remaining parameters of \tf{gemm}.

\begin{table}[t]
%\captionsetup{width=0.7\textheight}
\centering
\footnotesize
%\scriptsize
\begin{tabular}{ c c c c c c c c c c c c c c } % 
\toprule
Case \ & Order $p$ \ & Layout $\mbpi$ \ & Mode $q$ & Routine & \tf{T} & \tf{M} & \tf{N} & \tf{K} & \tf{A} & \tf{LDA} & \tf{B} & \tf{LDB} & \tf{LDC} \\
\midrule
1 & $1$ & -       & $1$      & \tf{gemv} & -       & $m$   & $n_1$ & -     & $\mbB$  & $n_1$ & $\mubA$  & - & - \\
\midrule
2 & $2$ & $(1,2)$ & $1$      & \tf{gemm} & $\mbB$  & $n_2$ & $m$   & $n_1$ & $\mubA$ & $n_1$ & $\mbB$   & $n_1$ & $m$   \\
3 & $2$ & $(1,2)$ & $2$      & \tf{gemm} & -       & $m$   & $n_1$ & $n_2$ & $\mbB$  & $n_2$ & $\mubA$  & $n_1$ & $n_1$ \\
4 & $2$ & $(2,1)$ & $1$      & \tf{gemm} & -       & $m$   & $n_2$ & $n_1$ & $\mbB$  & $n_1$ & $\mubA$  & $n_2$ & $n_2$ \\
5 & $2$ & $(2,1)$ & $2$      & \tf{gemm} & $\mbB$  & $n_1$ & $m$   & $n_2$ & $\mubA$ & $n_2$ & $\mbB$   & $n_2$ & $m$   \\
\midrule
6 & $>2$ & any    & $\pi_1$  & \tf{gemm} & $\mbB$  & $\mbnq$ & $m$     & $n_q$ & $\mubA$ & $n_q$ & $\mbB$  & $n_q$ & $m$\\
7 & $>2$ & any    & $\pi_p$  & \tf{gemm} & -       & $m$     & $\mbnq$ & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $\mbnq$ & $\mbnq$ \\
\midrule
%8 & $>2$ & any & \ $\pi_2,..,\pi_{p-1}$ \ & \tf{gemm*} & - & $m$ & $w_q$  & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $w_q$  & $w_q$ \\
8 & $>2$ & any & \ $\pi_2,..,\pi_{p-1}$ \ & \tf{gemm*} & - & $m$ & $n_{\pi_1}$  & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $w_{q}$  & $w_{q}$ \\
\bottomrule \\
\end{tabular}
%\vspace{0.2cm}
\caption%
{%
\footnotesize
Parameter configuration of the \tf{GEMV}- and \tf{gemm} routines with eight cases computing a tensor-matrix product in which $\mbB$ has the row-major format.
The BLAS arguments \tf{T}, \tf{M}, \tf{N}, etc. are chosen with respect to the tensor order $p$, tensor layout $\mbpi$ and contraction mode $q$ where \tf{T} specifies if $\mbB$ is transposed.
\tf{gemm*} denotes multiple \tf{gemm} calls with different tensor slices.
The number of rows for case 6 and 7 is given by $\bar{n}_q = (n_1 \cdots n_p) / n_q$.
\vspace{-0.5cm}
}
\label{tab:mapping}
\end{table}%\textbf{}

%We apply highly optimized routines to fully or partly execute tensor contractions as it is done in \cite{li:2015:input, shi:2016:tensor.contraction}.
%The function and parameter configurations for the tensor multiplication can be divided into eight cases.

%Table \ref{tab:mapping} extends the finding in \cite{dinapoli:2014:towards.efficient.use} precisely defining the mapping for any storage format. 
%It also complies with the findings in \cite{li:2015:input}.

%todo: need to decide which storage format the input and output arguments will have. row- or column-major <- this can be done later?

%todo: Mention that the following description is choosing the row-major interpretation of the 


\tit{Case 1 $(p=1)$:}
The tensor-vector product $\mubA \times_1 \mbB$ can be computed with a \tf{GEMV} operation $\mba^T \cdot \mbB$ where $\mubA$ is an order-$1$ tensor, i.e. a vector $\mba$ of length $n_1$.

\tit{Case 2-5 $(p=2)$:}
If $\mubA$ and $\mubC$ are order-$2$ tensors, i.e. a matrix $\mbA$ with dimensions $n_1$ and $n_2$, then a single \tf{gemm} suffices to compute the tensor-matrix product. 
If $\mbA$ and $\mbC$ have the column-major format with $\mbpi=(1,2)$, \tf{gemm} either executes $\mbC = \mbA \cdot \mbB^T$ for $q =1$ or $\mbC = \mbB \cdot \mbA$ for $q=2$.
Note that \tf{gemm} interprets $\mbC$ and $\mbA$ as matrices using the reshaping operation $\rho$ with $\mbrho = (2,1)$ in row-major format even though both are stored column-wise.
If $\mbA$ and $\mbC$ have the row-major format with $\mbpi=(2,1)$, \tf{gemm} either executes $\mbC = \mbB \cdot \mbA$ for $q =1$ or $\mbC = \mbA \cdot \mbB^T$ for $q=2$. 
Note that the transposition of $\mbB$ is necessary for the cases 2,5 and independent of the chosen storage format.

\tit{Case 6-7 $(p>2)$:}
If the order of $\mubA$ and $\mubC$ is greater than $2$ and if the contraction mode $q$ is equal to $\pi_1$ (case 6), a single \tf{gemm} with the depicted parameters executes $\mbC = \mbA \cdot \mbB^T$ and computes a tensor-matrix product $\mubC = \mubA \times_{\pi_1} \mbB$ for any storage layout of $\mubA$ and $\mubC$.
Tensors $\mubA$ and $\mubC$ are flattened with $\varphi_{2,p}$ to row-major matrices $\mbA$ and $\mbC$.
%$f_{2,p}$, see subsection \ref{sec:preliminaries:flattening}.
Matrix $\mbA$ has $\bar{n}_{\pi_1} = \bar{n} / n_{\pi_1}$ rows and $n_{\pi_1}$ columns while matrix $\mbC$ has the same number of rows and $m$ columns.
If $\pi_p=q$ (case 7), Tensors $\mubA$ and $\mubC$ are flattened with $\varphi_{1,p-1}$ to column-major matrices $\mbA$ and $\mbC$.
Matrix $\mbA$ has $n_{\pi_p}$ rows and $\bar{n}_{\pi_p} =  \bar{n} / n_{\pi_p}$ columns while matrix $\mbC$ has $m$ rows and the same number of columns.
A single \tf{gemm} executes $\mbC = \mbB \cdot \mbA$ and computes the tensor-matrix product $\mubC = \mubA \times_{\pi_p} \mbB$ for any storage layout of $\mubA$ and $\mubC$.
Note that in all cases no copy operation is performed in order to compute the desired contraction, see subsection \ref{sec:preliminaries:flattening.reshaping}.

\tit{Case 8 $(p>2)$:}
If the tensor order is greater than $2$ with $\pi_1\neq q$ and $\pi_p \neq q$, the modified baseline algorithm \ref{alg:ttm.sequential.coalesced} is used to successively call $\bar{n} / (n_q \cdot n_{\pi_1})$ times \tf{gemm} with different tensor slices of $\mubC$ and $\mubA$ in the base case.
Each \tf{gemm} computes one slice $\mubC_{\pi_1,q}'$ of the tensor-matrix product $\mubC$ using the corresponding tensor slices $\mubA_{\pi_1,q}'$ and the matrix $\mbB$.
The matrix-matrix product $\mbC = \mbB \cdot \mbA$ is performed by interpreting both tensor slices as row-major matrices $\mbA$ and $\mbC$ which have the dimensions $(n_q,n_{\pi_1})$ and $(m,n_{\pi_1})$, respectively.
%todo: note that the leading dimensions of A and C must be wq 

\subsection{BLAS-Based Algorithms with Subtensors}
\label{sec:design:blas.based.algorithm}
It is possible to further optimize case 8 by selecting larger subtensors instead of slices and enable higher processor utilization by executing multiple \tf{gemm}s with larger matrices.
Note that the base case of the modified baseline algorithm \ref{alg:ttm.sequential.coalesced} calls slice-matrix multiplication with slices of which two dimensions are greater than one, i.e. $n_q,n_{\pi_1}$ and $m,n_{\pi_1}$ for $\mubA$ and $\mubC$, respectively.
In order to use larger subtensors, additional mergeable modes must be selected that still allow the subtensor to be flattened into a matrix without reordering tensor elements, see section \ref{sec:preliminaries:flattening.reshaping}.
The maximum number of mergeable modes is $\mhq-1$ with $\mhq = \mbpi^{-1}(q)$ and the corresponding modes are $\pi_1,\pi_2,\dots,\pi_{\mhq-1}$.
Applying flattening $\varphi_{1,q-1}$ and reshaping $\rho$ with $\mbrho = (2,1)$ on a subtensor of $\mubA$ with dimensions $n_{\pi_1},\dots,n_{\pi_{\mhq-1}}, n_{q}$ yields a row-major matrix $\mbA$ with shape $(n_q,\prod_{r=1}^{\mhq-1} n_{\pi_r})$.
This is done analogously for $\mubC$ resulting in a row-major matrix with shape $(m, \prod_{r=1}^{\mhq-1} n_{\pi_r})$.

Algorithm \ref{alg:ttm.sequential.coalesced} needs a minor modification that allow each \tf{gemm} to be called with flattened subtensors of $\mubA$ and $\mubC$. 
The modified algorithm is omitted the first $\mhq$ modes $\mbpi_{1,\mhq} = (\pi_1, \dots, \pi_{\mhq})$ including $\pi_{\mhq} = q$.
This is done by only iterating over modes in the non-base case of the algorithm that are larger than $\mhq$.
The conditions in line 2 and 4 are therefore changed to $1 < r \leq \mhq$ and $\mhq < r$, respectively.
The single indices of the subtensors $\mubA_{\mbpi_{1,\mhq}}'$ and $\mubC_{\mbpi_{1,\mhq}}'$ within the base case of the algorithm are given by the loop induction variables that belong to the $\pi_r$-th loop with $\mhq+1 \leq r \leq p$.  %$i_{\pi_{\mhq+1}},\dots,i_{\pi_{p}}$.
Subtensor elements are contiguously stored and the number of non-unit dimensions is equal to $\mhq$.
%Hence, \tf{gemm} is called with flattened and reshaped subtensors of $\mubA$ and $\mubC$.

 
\subsection{Parallel BLAS-based Algorithms}
\label{subsec:parallel.multi-loops}
Note that cases 1 to 7 only call a single \tss{gemm} and cannot be further parallelized.
Hence, we focus on the previously described algorithm which sequentially calls multi-threaded \tf{gemm}s.
This is beneficial if $q = \pi_{p-1}$, the inner dimensions $n_{\pi_1},\dots,n_{q}$ are large or the outer-most dimension $n_{\pi_{p}}$ is smaller than the available processor cores.
If the above conditions are not met, the processor cores might fully utilized where each multi-threaded \tf{gemm} is executed with small subtensors.
The algorithm will be referred to as the \tbf{multithreaded} \tbf{gemm} with \tbf{subtensors} or tensor \tbf{slices}.

A more advanced version of the above algorithm is to execute single-threaded \tf{gemm}s in parallel including all available (free) modes for the parallelization.
The general subtensor-matrix approach contains $\pi_{\mhq+1}, \dots, \pi_{p}$ free modes whereas in case of the slice-matrix multiplications, all modes except $\pi_1$ and $\pi_{\mhq}$ are free.
%A simple approach is to parallelize the outer-most loop $\pi_p$ using the OpenMP \tf{parallel} \tf{for} directive which allows OpenMP threads to execute their own sequential subtensor-matrix multiplications independently.
%However, the degree of parallelism is limited by the outer-most dimension $n_{\pi_p}$.
%Hence, more available modes need to be considered to increase parallelism.
Their respective maximum degree of parallelism is $\prod_{r=\mhq+1}^{p} n_{\pi_r}$ and $\prod_{r=1}^{p} n_{r} / (n_{\pi_1} n_{\pi_{\mhq}})$.
Note that the number of free modes depends on the tensor order $p$ and contraction mode $q$.
%
\begin{comment}
%note: did not implement this one.
We have placed the \tf{taskloop} directive in front of the loop in line 5 which allows to partition all free loops into tasks for parallel execution.
This allows to collapse iterations of all free loops within the desired recursion levels into one larger divisible iteration space.
One advantage of this approach is that both tensors or subtensors do not need to be flattened or reshaped and the existing algorithm can be reused.
We will refer to as the \tbf{taskloops} over \tbf{sequential} \tbf{gemm} with subtensors or slices.
\end{comment}

The corresponding free loops can be executed in parallel using the OpenMP worksharing directives by flattening (collapsing) all loops within the tree-recursion into one or two loops depending on the available fusible loops.
%fuse available free loops and then to parallelize the resulting loop using either the OpenMP \tf{parallel for} directive with flattened subtensors or to directly call batched matrix multiplication routines, e.g. \tf{BLAS\_batched\_gemm} if such a rountine available.
Considering the slice-matrix approach, both tensors are flattened twice with $\varphi_{\pi_{\mhq+1},\pi_p}$ and $\varphi_{\pi_{2},\pi_{\mhq-1}}$. 
The resulting tensor is of order $4$ with dimensions $n_{\pi_1}, \mhn_{\pi_2}, n_{q}, \mhn_{\pi_4}$ where $\mhn_{\pi_2} = \prod_{r=2}^{\mhq-1} n_{\pi_r}$ and $\mhn_{\pi_4} = \prod_{r=\mhq+1}^{p} n_{\pi_r}$.
The corresponding algorithm consists of two nested loops.
The outer loop iterates over $\mhn_{\pi_4}$ while the inner loop iterates over $\mhn_{\pi_2}$ calling \tf{gemm} with multiple slices $\mubA_{\pi_1,q}'$ and $\mubC_{\pi_1,q}'$.
The two loops are parallelized using the \tf{parallel for} together with the \tf{collapse(2)} and the \tf{num\_threads} clause.
The latter specifies the number of threads used within the parallel OpenMP region.
%todo: are we doing some processor binding?
%todo: what is the scheduling policy? schedule(static) seems to be 
In case of the general subtensor-matrix approach, both tensors are flattened twice with $\varphi_{\pi_{\mhq+1},\pi_p}$ and $\varphi_{\pi_{1},\pi_{\mhq-1}}$. 
The resulting tensor is of order $3$ with dimensions $\mhn_{\pi_1}, n_{q}, \mhn_{\pi_4}$ where $\mhn_{\pi_1} = \prod_{r=1}^{\mhq-1} n_{\pi_r}$ and $\mhn_{\pi_4} = \prod_{r=\mhq+1}^{p} n_{\pi_r}$.
The corresponding algorithm consists of one loops which iterates over $\mhn_{\pi_4}$ calling single-threaded \tf{gemm} with multiple subtensors $\mubA_{\mbpi',q}'$ and $\mubC_{\mbpi',q}'$ with $\mbpi' = (\pi_1,\dots,\pi_{\mhq-1})$.
We will refer to as the \tbf{parallel} \tbf{loops} over \tbf{sequential} \tbf{gemm} with subtensors or tensor slices.
We have also used 

The next algorithm is a modified version of the general subtensor-matrix approach that uses a single batched \tf{gemm} for the eighth case.
We will refer to as the \tbf{batched} \tbf{gemm} with subtensors.
%todo: need to provide the parameter calls.
