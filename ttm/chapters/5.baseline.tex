\section{Algorithm Design}
\label{sec:design}
\subsection{Sequential Baseline Algorithm}
\label{sec:design:sequential.baseline.algorithm}
The sequential baseline algorithm implementing Eq. \ref{equ:tensor.matrix.multplication} can be implemented with a single C++ function.
It consists of nested recursion with a control flow that resembles algorithm 1 in \cite{bassoy:2018:fast}, consisting of two \ttt{if} statements with an \ttt{else} branch.
The body of the first \ttt{if} statement contains a recursive call that skips the iteration over the dimension $n_{q}$ when $r = \mhq$ with $\pi_r = q$ and $\mhq = \mbpi^{-1}_q$ where $\mbpi^{-1}$ is the inverse layout tuple.
%TODO: check if q or \mhq
% with 
The second \ttt{if} statement contains multiple recursive calls for the modes $1 \leq r \neq \mhq \leq p$ with different multi-indices.
Note that the second \ttt{if} statement is skipped for $q = \pi_1$ as the condition of the first one is evaluated to true.
The \ttt{else} branch is the base case and consists of two loops that compute a fiber-matrix product.
The inner loop iterates over the dimension $n_q$ of $\mubA$ and $\mbB$ with index $1\leq i_q \leq n_q$ computing an inner product.
The outer loop iterates over the dimension $m$ of $\mubC$ and $\mbB$ with index $1 \leq j \leq m$.
The baseline algorithm supports tensors with arbitrary order, dimensions and any non-hierarchical storage format.

\subsection{Modified Baseline Algorithm with Contiguous Memory Access}
\label{sec:design:modified.baseline.algorithm}
The baseline algorithm accesses memory of $\mubA$ and $\mubC$ non-contiguously whenever $\pi_1 \neq q$ so that indices $i_q$ and $j$ are incremented with steps greater than one.
Matrix $\mbB$ is contiguously accessed if $i_q$ or $j$ is incremented with unit-steps depending on the storage format of $\mubB$.
The access pattern could be improved by reordering tensor elements according to the storage format which results in copy operations reducing the overall throughput of the operation \cite{shi:2016:tensor.contraction}.

A better approach is to access tensor elements according to the tensor layout using the permutation tuple $\mbpi$ as proposed in \cite{bassoy:2018:fast}.
The modified algorithm with contiguous memory accesses is given in algorithm \ref{alg:ttm.sequential.coalesced} for $\pi_1 \neq q$ and $p > 1$.
Each recursion level adjusts only one multi-index element $\mbi(\pi_r)$ with a stride $\mbw(\pi_r)$ as depicted in line 5.
With increasing recursion level and decreasing $r$, indices are incremented with smaller step sizes as $\mbw(\pi_r) \leq \mbw(\pi_{r+1})$. 
The condition of the second \ttt{if} statement in line 4 is changed from $r \geq 1$ to $r > 1$.
In this way, the loop incrementing with index $\mbi(\pi_1)$ and the minimum stride $\mbw(\mbpi_1)$ can be included in the base case which contains three loops performing a slice-matrix multiplication. 
The ordering of the three loops within the base case are adjusted according to the tensor and matrix layout.
The inner-most loop increments $\mbi(\pi_1)$ and therefore contiguously accesses tensor elements of $\mubA$ and $\mubC$.
The second loop increments $i_q$ with which elements of $\mbB$ are contiguously accessed if $\mbB$ is stored in the row-major format.
The third loop increments $j$ and could be placed as the second loop if $\mbB$ is stored in the column-major format.
The simple ordering of the three loops is discussed in \cite{golub:2013:matrix.computations}.

\begin{algorithm}[t]
%\SetAlgoNoLine
\DontPrintSemicolon
\SetKwProg{Fn}{}{}{end}
\SetKwFunction{function}{tensor\_times\_matrix}%
%\SetAlgoNoEnd
\footnotesize 
\SetAlgoVlined
\hrule
\BlankLine
\Fn{\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r$}}
{
	\uIf{$r = \mhq$ }
	{
		\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$ }
	}
	\uElseIf{$r > 1$ }
	{
		\For{$i_{\pi_r} \leftarrow 1$ \KwTo $ n_{\pi_r}$}
		{
			\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$}\;
		}		
	}	
	\Else%{$r\geq1 \wedge m \neq 1$}
	{
		\For{$j \leftarrow 1$ \KwTo $m$}
		{
			\For{$i_q \leftarrow 1$ \KwTo $n_q$}
			{			
				\For{$i_{\pi_1} \leftarrow 1$ \KwTo $n_{\pi_1}$}
				{
					$\mubC(i_1,...,i_{q-1},j,i_{q+1},...,i_{p})$ \ttt{+=} $\mubA(i_1,...,i_q,...,i_p) \cdot \mbB(j,i_q)$\;
				}
			}
		}
	}
}
\BlankLine
\hrule
\caption{
\footnotesize %
Modified baseline algorithm with contiguous memory access for the tensor-matrix multiplication.
The tensor order must be greater than one and for the contraction mode $1 \leq q \leq p$ and $\pi_1 \neq q$ must hold.
The algorithm needs to be initially called with $r=p$ where $\mbn$ is the shape tuple of $\mubA$ and $m$ is the $q$-th dimension of $\mubC$. 
%Iteration along mode $q$ with $\mhq = \mbpi^{-1}_q$ is moved into the inner-most recursion level.
\label{alg:ttm.sequential.coalesced}
}
\end{algorithm}

While the spatial data locality is improved by adjusting the loop ordering, the temporal data locality of tensors $\mubA$ and $\mubC$ differ.
Note that slice $\mubA_{\pi_1,q}'$ is accessed $m$ times, fiber $\mubC_{\pi_1}$ is accessed $\mbn(q)$ times and element $\mubB(j,i_q)$ is accessed $\mbn(\pi_1)$ times.
While the specified fiber of $\mubC$ can fit into first or second level cache, slice elements of $\mubA$ are unlikely to fit in the local caches if the slice size $n_{\pi_1} \times n_q$ is large leading to higher cache misses and suboptimal performance.
%TODO: hier mehr paper, die BLAS Implementierung referenzieren?
Optimized tiling for better temporal data locality has been discussed in \cite{goto:2008:gemm} which suggests to use existing high-performance BLAS implementations for the base case.
%The proposed algorithm therefore constitutes the starting point for \tf{BLAS} utilization within the base case.

\subsection{GEMM-based Algorithms}
\label{sec:design:blas.based.algorithm}
%\vspace{-0.3em}
The proposed algorithm \ref{alg:ttm.sequential.coalesced} is the starting point for the BLAS-based algorithm which computes the tensor-matrix product with a \tf{GEMM} routine.
Besides the illustrated algorithm, we have identified seven other cases where a single \tf{GEMM} call suffices to compute the tensor-matrix product even if the tensor order $p$ is greater than two.
In summary there are eight cases with a single \tf{GEMM} call using different arguments which are listed in table \ref{tab:mapping}.
The table is complete with no limitation on tensor order and contraction mode, supporting all linear tensor layout.
\tf{GEMM} arguments are chosen depending on the tensor order $p$, tensor layout $\mbpi$ and contraction mode $q$ except for the \tf{CBLAS\_ORDER} which is \tf{CblasRowMajor}.
%Note , all linear tensor layouts can be supported by setting the remaining parameters of \tf{GEMM}.

\begin{table}[t]
%\captionsetup{width=0.7\textheight}
\centering
\footnotesize
%\scriptsize
\begin{tabular}{ c c c c c c c c c c c c c c } % 
\toprule
Case \ & Order $p$ \ & Layout $\mbpi$ \ & Mode $q$ & Routine & \tf{T} & \tf{M} & \tf{N} & \tf{K} & \tf{A} & \tf{LDA} & \tf{B} & \tf{LDB} & \tf{LDC} \\
\midrule
1 & $1$ & -       & $1$      & \tf{GEMV} & -       & $m$   & $n_1$ & -     & $\mbB$  & $n_1$ & $\mubA$  & - & - \\
\midrule
2 & $2$ & $(1,2)$ & $1$      & \tf{GEMM} & $\mbB$  & $n_2$ & $m$   & $n_1$ & $\mubA$ & $n_1$ & $\mbB$   & $n_1$ & $m$   \\
3 & $2$ & $(1,2)$ & $2$      & \tf{GEMM} & -       & $m$   & $n_1$ & $n_2$ & $\mbB$  & $n_2$ & $\mubA$  & $n_1$ & $n_1$ \\
4 & $2$ & $(2,1)$ & $1$      & \tf{GEMM} & -       & $m$   & $n_2$ & $n_1$ & $\mbB$  & $n_1$ & $\mubA$  & $n_2$ & $n_2$ \\
5 & $2$ & $(2,1)$ & $2$      & \tf{GEMM} & $\mbB$  & $n_1$ & $m$   & $n_2$ & $\mubA$ & $n_2$ & $\mbB$   & $n_2$ & $m$   \\
\midrule
6 & $>2$ & any    & $\pi_1$  & \tf{GEMM} & $\mbB$  & $\mbnq$ & $m$     & $n_q$ & $\mubA$ & $n_q$ & $\mbB$  & $n_q$ & $m$\\
7 & $>2$ & any    & $\pi_p$  & \tf{GEMM} & -       & $m$     & $\mbnq$ & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $\mbnq$ & $\mbnq$ \\
\midrule
%8 & $>2$ & any & \ $\pi_2,..,\pi_{p-1}$ \ & \tf{GEMM*} & - & $m$ & $w_q$  & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $w_q$  & $w_q$ \\
8 & $>2$ & any & \ $\pi_2,..,\pi_{p-1}$ \ & \tf{GEMM*} & - & $m$ & $n_{\pi_1}$  & $n_q$ & $\mbB$ & $n_q$ & $\mubA$  & $n_{\pi_1}$  & $n_{\pi_1}$ \\
\bottomrule \\
\end{tabular}
%\vspace{0.2cm}
\caption%
{%
\footnotesize
Parameter configuration of the \tf{GEMV}- and \tf{GEMM} routines with eight cases computing a tensor-matrix product.
The routine arguments are chosen with respect to the tensor order $p$, tensor layout $\mbpi$ and contraction mode $q$ which determine the \tf{GEMM} arguments for \tf{T}, \tf{M} to \tf{LDC}.
The parameter \tf{T} denotes the transposition for matrix $\mbB$. 
The routine \tf{GEMM*} denotes multiple \tf{GEMM} calls with different tensor slices.
%In case of order-$2$ and order-$q$ slices, the number of rows must be equal to $\mhn_q = n_{\pi_1}$ and $\mhn_q =w_q$, respectively.
The number of rows for case 6 and 7 is given by $\bar{n}_q = \bar{n} / n_q$ with $\bar{n} = n_1 \cdots n_p$.
%\vspace{-0.5cm}
}
\label{tab:mapping}
\end{table}%\textbf{}

%TODO: comment on previous impls maybe later in this section.
%We apply highly optimized routines to fully or partly execute tensor contractions as it is done in \cite{li:2015:input, shi:2016:tensor.contraction}.
%The function and parameter configurations for the tensor multiplication can be divided into eight cases.

%\todo[noline]{Could we do also tensor transposition?}.
%Table \ref{tab:mapping} extends the finding in \cite{dinapoli:2014:towards.efficient.use} precisely defining the mapping for any storage format. 
%It also complies with the findings in \cite{li:2015:input}.

%TODO: need to decide which storage format the input and output arguments will have. row- or column-major <- this can be done later?

%TODO: Mention that the following description is choosing the row-major interpretation of the 


\tit{Case 1 $(p=1)$:}
The tensor-vector product $\mubA \times_1 \mbB$ can be computed with a \tf{GEMV} operation $\mba^T \cdot \mbB$ where $\mubA$ is an order-$1$ tensor, i.e. a vector $\mba$ of length $n_1$.

\tit{Case 2-5 $(p=2)$:}
If $\mubA$ and $\mubC$ are order-$2$ tensors, i.e. a matrix $\mbA$ with dimensions $n_1$ and $n_2$, then a single \tf{GEMM} suffices to compute the tensor-matrix product. 
If $\mbA$ and $\mbC$ have the column-major format with $\mbpi=(1,2)$, \tf{GEMM} either executes $\mbC = \mbA \cdot \mbB^T$ for $q =1$ or $\mbC = \mbB \cdot \mbA$ for $q=2$.
Note that \tf{GEMM} interprets $\mbC$ and $\mbA$ as matrices in row-major format even though both are stored column-wise.
If $\mbA$ and $\mbC$ have the row-major format with $\mbpi=(2,1)$, \tf{GEMM} either executes $\mbC = \mbB \cdot \mbA$ for $q =1$ or $\mbC = \mbA \cdot \mbB^T$ for $q=2$. 
Note that the transposition of $\mbB$ is necessary for the cases 2,5 and independent of the chosen storage format.

\tit{Case 6-7 $(p>2)$:}
If the order of $\mubA$ and $\mubC$ is greater than $2$ and if the contraction mode $q$ is equal to $\pi_1$ (case 6), a single \tf{GEMM} with the depicted parameters executes $\mbC = \mbA \cdot \mbB^T$ and computes a tensor-matrix product $\mubC = \mubA \times_{\pi_1} \mbB$ for any storage layout of $\mubA$ and $\mubC$.
Tensors $\mubA$ and $\mubC$ are flattened to matrices $\mbA$ and $\mbC$ by applying $\varphi_{2,p}$ 

$f_{2,p}$, see subsection \ref{sec:preliminaries:flattening}.
Matrix $\mbA$ has $\mbnq = \bar{n} / n_q$ rows and $n_q$ columns while matrix $\mbC$ has the same number of rows and $m$ columns.
If $\pi_p=q$, the \tf{GEMM} executes $\mbC = \mbB \cdot \mbA$ and computes a tensor-matrix product $\mubC = \mubA \times_{\pi_p} \mbB$ for any storage layout of $\mubA$ and $\mubC$.
The matrix $\mbA$ has $n_q$ rows and $\mbnq$ rows while matrix $\mbC$ has $m$ rows and the same number of columns.
Note that in all cases no copy operation is performed in order to compute the desired contraction.

\tit{Case 8 $(p>2)$:}
If the tensor order is greater than $2$, $\pi_1\neq q$ and $\pi_p \neq q$, the modified baseline algorithm \ref{alg:ttm.sequential.coalesced} is used to successively call $\bar{n} / (n_q \cdot n_{\pi_1})$ times \tf{GEMM} with different tensor slices of $\mubC$ and $\mubA$ in the base case.
Each \tf{GEMM} computes one slice $\mubC_{\pi_1,q}'$ of the tensor-matrix product $\mubC$ using the corresponding tensor slices $\mubA_{\pi_1,q}'$ and the matrix $\mbB$.
The matrix-matrix product $\mbC = \mbB \cdot \mbA$ is performed by interpreting both tensor slices as matrices $\mbC$ and $\mbA$ which have the dimensions $(n_{\pi_1},m)$ and $(n_q,n_{\pi_1})$, respectively.


\begin{comment}
%Algorithm \ref{alg:ttm.sequential.coalesced} calls $\prod_{r=1}^p n_q / (n_q \cdot n_{\pi_1})$ \tf{GEMM} with different tensor slices $\mubA'$, $\mubC'$ and the same matrix $\mbB$.

Lines 8 to 10 of Algorithm \ref{alg:ttv.sequential.blocked} perform a slice-vector multiplication of the form $\mbc' = \mbA' \cdot \mbb$.
It is executed with a \tf{GEMV} with no further adjustment of the algorithm.
The vector $\mbc'$ denotes a fiber of $\mubC$ with $n_u$ elements and $\mbA'$ denotes an order-$2$ slice of $\mubA$ with dimensions $n_u$ and $n_v$ such that
\begin{equation}
\label{equ:order.2.slice}
\mbA' = \mubA(i_1,\dots,\colon_{\mkern-5.5muu},\dots,\colon_{\mkern-5.5muv},\dots,i_p) 
\quad \text{and} \quad
\mbc' = \mubC(i_1,\dots,\colon_{\mkern-5.5muu},\dots,i_p)
\end{equation}
where $u=\pi_1$ and $v = q$ or vice versa.
%\vspace{-0.3cm}
%\paragraph{Multi-Loop (v2) with Order-$\mhq$ Slices:}
Algorithm \ref{alg:ttv.sequential.blocked} needs a minor modification in order to loop over order-$\mhq$ slices. 
With $\mhq = (\pi^{-1})_q$, the conditions in line 2 and 4 are changed to $1 < r \leq \mhq$ and $\mhq < r$, respectively.
The modified algorithms therefore omits the first $\mhq$ modes $\pi_1, \dots, \pi_{\mhq}$ including $\pi_{\mhq} = q$ where all elements of an order-$\mhq$ slice are contiguously stored.
Choosing the first-order storage format for convenience, the order-$\mhq$ and order-$(\mhq-1)$ slices of both tensors are given by 
\begin{equation}
\label{equ:order.mhq.slice}
\mubA' = \mubA(\colon_{\mkern-5.5mu1},\dots,\colon_{\mkern-5.5muq},i_{q+1},\dots,i_{p}) 
\ \text{and} \
\mubC' = \mubC(\colon_{\mkern-5.5mu1},\dots,\colon_{\mkern-5.5mu{q-1}},i_{q+1},\dots,i_p).
\end{equation}
The fiber $\mbc'$ of length $w_q = n_{1} \cdot n_2 \cdots n_{q-1}$ is the one-dimensional interpretation of $\mubC'$ and the order-$2$ slice $\mbA'$ with dimensions $w_q$ and $n_q$ the two-dimensional interpretation of $\mubA'$.
The slice-vector multiplication in this case can be performed with a \tf{GEMV} that interprets the order-$\mhq$ slices as order-$2$ according to the description.
\end{comment}


\subsection{Modified Baseline Algorithms with Adjustable Subtensor Size}
\label{sec:design:blas.based.algorithm}

Case 1-7 cannot be further optimized.
Case 8 uses the modified baseline algorithm 1 to call slice-matrix multiplication.

Enlarge tensor slice and include more modes for the non-contracting mode in order to have slice-matrix multiplications with more elements.

What can be $(1,..,\pi_1,..,q,..,p)$ and $(1,..,q,..,\pi_1,..,p)$

Take $\pi_1,..,q$ and 


\subsection{Parallel Algorithms with Slice-Vector Multiplications}
\label{subsec:parallel.multi-loops}
A straight-forward approach for generating a parallel version of Algorithm \ref{alg:ttv.sequential.blocked} is to divide the outer-most $\pi_p$-th loop into equally sized iterations and execute them in parallel using the \tf{OpenMP} \tf{parallel} \tf{for} directive\cite{bassoy:2018:fast}.
%parallelize the multi-loops in case 8 is described by \cite{bassoy:2018:fast} where the outer-most $\pi_p$-th loop is divided into chunks of the same length. 
With no critical sections and synchronization points, all threads within the parallel region execute their own sequential slice-vector multiplications.
The outer-most dimension $n_{\pi_p}$ determines the degree of parallelism, i.e. the number of parallel threads executing their own instruction stream.


%Yet, the number of parallelizable dimensions are dynamic and depend on the contraction mode $q$ and the storage layout of the tensor.
%Considering Algorithm \ref{alg:ttv.sequential.blocked} executing slice-vector multiplications with order-$2$ slices.
%The degree of parallelism can be improved by e.g. selecting other or additional loops.
Fusing additional loops into a single one improves the degree of parallelism.
%that are not required by the slice-vector multiplication in order to achieve a higher degree of parallelism.
The number of fusible loops depends on the tensor order $p$ and contraction mode $q$ of the tensor-vector multiplication with $\mhq = (\pi^{-1})_q$.
In case of mode-$q$ slice-vector multiplications, loops $\pi_{\mhq+1}, \dots, \pi_{p}$ are not involved in the multiplications and can be transformed into one single loop.
For mode-$2$ slice-vector multiplications all loops except $\pi_1$ and $\pi_{\mhq}$ can be fused.
%Moreover, the fused loops are not required by a slice-vector multiplication.
When all fusible loops are lexically present and both parameters are known before compile time, loop fusion and parallel execution can be easily accomplished with the \tf{OpenMP} \tf{collapse} directive.
The authors of \cite{li:2015:input} use this approach to generate parallel tensor-matrix functions.

%\paragraph{Multi-Loop with Order-$\mhq$ Slices}
%The parallelization of additional loops yields a higher degree of parallelism.
With variable number of dimensions and a variable contraction mode, the iteration count of slice-vector multiplications and the slice selection needs to be determined at compile or run time.
%The number of iterations of the fused loop is determined by the product of the outer dimensions and the increment is given by the slice size.
If $\bar{n}$ is the number of tensor elements of $\mubA$, the total number of slice-vector multiplications with mode-$\mhq$ slices is given by $\bar{n}' = \bar{n} / w_{q}$.
%\begin{equation}
%\begin{split}
%\bar{n}' & = n_{\pi_{k+1}} \cdot n_{\pi_{k+2}} \cdots n_{\pi_{p}} \\
%         & = \bar{n} / w_{m}
%\end{split}
%\end{equation}
%$\bar{n}_{k+1} = n_{\pi_{k+1}} \cdot n_{\pi_{k+2}} \cdots n_{\pi_{p}}$.
Using Eq. \eqref{equ:stride.tuple}, the strides for the iteration are given by $w_{\pi_{\mhq+1}}$ for $\mubA$ and $v_{\pi_{\mhq}}$ for $\mubC$.
In summary, one single parallel outer loop with an iteration count $\bar{n}'$ and an increment variable $j$ iteratively calls mode-$\mhq$ slice-vector multiplications with adjusted memory location $j \cdot w_{\pi_{\mhq+1}}$ and $j \cdot v_{\pi_{\mhq}}$ for $\mubA$ and $\mubC$, respectively.
The degree of parallelism $\prod_{r=\mhq+1}^p n_r$ decreases with increasing $\mhq$ and corresponds for $\mhq = p-1$ to the first parallel version. 
Tensor-vector multiplications with mode-$2$ slice-vector multiplications are further optimized by fusing additional $\mhq-2$ loops.
%The layout, shape and stride tuples of $\mubA$ and $\mubC$ need to be divided in two parts.
%Those without the modes $\pi_1$ and $\pi_q$ are needed to select order-$2$ slices by transforming the iteration index to relative memory locations of an order-$2$ slice with $\lambda_{\mbv''} \circ \gamma \circ \lambda^{-1}_{\mbw''}$.\todo{still need to finish this.}


\begin{comment}
Then
\begin{equation}
\mbpsi = 
\begin{cases}
(1,2) & \text{if} \ \pi_1 < \pi_k\\
(2,1) & \text{otherwise.}
\end{cases}
\end{equation}
\end{comment}

\begin{comment}
\begin{equation}
\begin{array}{lllll}
\pi_i''     &=& \pi_i   &\quad \text{if} & \quad \pi_i < \pi_1 \wedge \pi_i < \pi_k \\
\pi_{i-1}'' &=& \pi_i-1 &\quad \text{if} & \quad (\pi_i > \pi_1 \wedge \pi_i < \pi_k) \vee (\pi_i < \pi_1 \wedge \pi_i > \pi_k) \\
\pi_{i-2}'' &=& \pi_i-2 &\quad \text{if} & \quad \pi_i > \pi_1 \wedge \pi_i > \pi_k \\
\end{array}
\end{equation}
\end{comment}





\begin{comment}
\begin{algorithm}[t]
%\SetAlgoNoLine
\DontPrintSemicolon
\SetKwProg{Fn}{}{}{end}
\SetKwFunction{TTV}{tensor\_times\_vector}%
\SetAlgoVlined
\hrule
\BlankLine
\Fn{\TTV{$\mubA, \mbb, \mubC, \mbn, \mbi, \mbpi, q, r$}}
{
\uIf{$r > 1$ }
{
\For{$i_{\pi_r} \leftarrow 1$ \KwTo $ n_{\pi_r}$}
{
\TTV{$\mubA, \mbb, \mubC, \mbn, \mbi, \mbpi, q, r-1$}\;
}		
}	
\Else%{$r\geq1 \wedge m \neq 1$}
{			
\For{$i_{\mbpi_1} \leftarrow 1$ \KwTo $n_{\mbpi_1}$}
{
$\mubC(i_1,\dots,i_{q-1},i_{q+1},\dots,i_{p})$ \ttt{+=} $\mubA(i_1,\dots,i_q,\dots,i_p) \cdot \mbb(i_q)$\;			
}
}
}
\BlankLine
\hrule
\caption{
\footnotesize Sequential version of the mode-$q$ tensor-vector multiplication  with contiguous memory access for tensors with any order $p\geq 2$ and any non-hierarchical storage format. The function initially called with $r=p$ recursively loops over the complete multi-index space of $\mubA$ and $\mubC$.
\label{alg:ttv.sequential.coalesced}
}
\end{algorithm}
\end{comment}
