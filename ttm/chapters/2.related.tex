\section{Related Work}
\label{sec:related}

\begin{comment}
The authors in \cite{dinapoli:2014:towards.efficient.use} discuss the efficient tensor contractions with highly optimized BLAS. 
%They describe a slicing technique of tensors for using BLAS. 
%With a set of requirements they define three contraction categories.
Based on the LoG approach, they define requirements for the use of \tf{gemm} for class 3 tensor contractions and provide slicing techniques for tensors. %  when both arguments exhibit free indices
The slicing recipe for the class 2 categorized tensor contractions contains a short description with a rule of thumb for maximizing performance.
%Compared to class 3 operations, the tensor-vector multiplication receives less attention.
Runtime measurements cover class 3 tensor contractions.
%todo: weg und was anderes, zum Beispiel mein eigenes Paper
\end{comment}

The authors of \cite{springer:2018:design} present a tensor-contraction generator TCCG and the GETT approach for dense tensor contractions that is inspired from the design of a high-performance \tf{gemm}.
Their unified code generator selects implementations from generated GETT, LoG and TTGT candidates.
Their findings show that among $48$ different contractions $15$\% of LoG-based implementations are the fastest.

The author presents in \cite{matthews:2018:high} a runtime flexible tensor contraction library that uses GETT approach as well.
He describes block-scatter-matrix algorithm which uses a special layout for the tensor contraction.
The proposed algorithm yields results that feature a similar runtime behavior to those presented in \cite{springer:2018:design}.

The work in \cite{li:2015:input} introduces InTensLi, a framework that generates in-place tensor-matrix multiplication according to the LOG approach. 
The authors discusses optimization and tuning techniques for slicing and parallelizing the operation.
With optimized tuning parameters, they report a speedup of up to $4$x over the TTGT-based MATLAB tensor toolbox library discussed in \cite{bader:2006:algorithm862}.
%Although many aspects are similar to our work, the authors emphasize the code generation of tensor-matrix multiplications using high-performance \tf{GEMM}'s.
%todo: hier besser sagen, welche Konzepte wir hierfür übernehmen.

In \cite{bassoy:2019:ttv}, the author presents LoG-based algorithms that compute the tensor-vector product. 
They support dense tensors with linear tensor layouts, arbitrary dimensions and tensor order.
The presented approach is to divide into eight cases calling \tf{gemv} and \tf{dot} routines.
He reports average speedups of 6.1x and 4.0x compared to implementations that use the TTGT and GETT approach, respectively.

Our work is inspired by \cite{li:2015:input} and \cite{bassoy:2019:ttv}.
We use the lemmas in \cite{li:2015:input} for our slicing technique
