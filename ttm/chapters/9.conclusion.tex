\section{Conclusion and Future Work}
\label{sec:conclusion}
We presented efficient layout-oblivious algorithms for the compute-bound tensor-matrix multiplication which is essential for many tensor methods.
Our approach is based on the loops-over-\tf{gemm} method and computes the tensor-matrix product in-place without transposing tensors.
It combined the flexible approach described in \cite{bassoy:2019:ttv} with the findings in \cite{li:2015:input}.
The resulting algorithms are able to process dense tensors with arbitrary tensor order, dimensions and with any linear tensor layout all of which can be runtime variable.
%They consist of eight cases each of which calls a highly-optimized BLAS with different arguments.
%We have discussed two slicing options and parallelization techniques to improve their performance for the eighth case.

Our benchmarks show that tiling the base algorithm into eight different \tf{gemm} cases improves the overall performance.
%, outperforming other state-of-the-art implementations, on average, by $50$\% or more for seven of the eight cases.
%todo: zahlen überprüfen.
%For the last general case, 
We have demonstrated that algorithms with parallel loops over single-threaded \tf{gemm} calls with tensor slices and subtensors perform best.
Interestingly, they outperform a single \tf{gemm\_batch} call with subtensors, on average, by $14$\% in case of asymmetrically shaped tensors and if tensor slices are used.
%todo: zahlen prüfen.
Both version computes the tensor-matrix product on average at least by $14.05$\% and up to a factor of $3.79$ faster than other state-of-the-art implementations.

%todo: We have demonstrated that the tensor layout does not negatively affect the performance of our algorithms.

Summarizing our findings, LOG-based tensor-times-matrix algorithms are able to outperform TTGT-based and GETT-based implementations without sacrificing on flexibility or maintainability.
Hence, other actively developed libraries such as LibTorch and Eigen will benefit from implementing the proposed algorithms.
Our header-only library provides C++ interfaces and a python module which allows frameworks to easily integrate our library.

In the future, we intend to generalize LOG-based approach for general tensor contractions with the same flexibility that we offered for the tensor-matrix multiplication. 
We would like to further optimize the tensor-matrix multiplication based on  benchmarks of matrix-matrix products which might lead to even better runtime results for edge cases.


\subsubsection{Source Code Availability}
Project description and source code can be found at {\footnotesize \MYhref[red]{https://github.com/bassoy/ttm}{https://github.com/bassoy/ttm}}.
The sequential tensor-matrix multiplication of \tf{TLIB} is part of \tf{uBLAS} and in the official release of \tf{Boost} \tf{v1.70.0} or later.
