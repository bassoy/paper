\section{Introduction}
\label{sec:introduction}
Tensor computations are found in many scientific fields such as computational neuroscience, pattern recognition, signal processing and data mining \cite{karahan:2015:tensor, papalexakis:2017:tensors}.
Tensors representing large amount of multidimensional data are decomposed and analyzed with the help of basic tensor operations \cite{lee:2018:fundamental, kolda:2009:decompositions}. 
%While tensor contraction with arithmetic intensity has been discussed, class 2 tensor operations 
The decomposition and analysis led to the development and analysis of high-performance kernels for tensor contractions.
In this work, we present and analyze a high-performance algorithm for the tensor-matrix multiplication that is used in many numerical algorithms such as the alternating least squares method \cite{lee:2018:fundamental, kolda:2009:decompositions}.
It is a compute-bound tensor operation and has the same arithmetic intensity as a matrix-matrix multiplication which can reach near peak performance of a computing machine. 
%%with free tensor indices, most implementations of the above mentioned approaches reach near peak performance of the computing machine\cite{springer:2018:design, matthews:2018:high,shi:2016:tensor.contraction}. 

To our best knowledge, there has been three main approach to implement tensor contractions.
The Transpose-Transpose-\tf{GEMM}-Transpose (\tf{TGGT}) approach reorganizes (flatens) tensors in order to perform a tensor contraction with an optimized matrix-matrix multiplication (\tf{GEMM}) implementation \cite{bader:2006:algorithm862,solomonik:2013:cyclops}.
Implementations of a more recent method (\tf{GETT}) are based on high-performance \tf{GEMM}-like algorithms \cite{springer:2018:design,matthews:2018:high,abadi:2016:tensorflow}.
A different method is the \tf{LOG} approach in which algorithms utilize \tf{GEMM} with multiple tensor slices if possible \cite{dinapoli:2014:towards.efficient.use, li:2015:input, shi:2016:tensor.contraction}.


%TODO: hier weiter und ganz klar differenzieren was noch gebraucht wird und weshalb
Our analysis is motivated by the observation that \tf{LOG} implementations of the tensor-matrix multiplication has not been fully thoroughly investigated. 
Our approach is akin to the one proposed in \cite{li:2015:input, shi:2016:tensor.contraction} but targets the utilization of general matrix-matrix multiplication routines (\tf{GEMM}) using \tf{OpenBLAS}, \tf{Intel MKL} and \tf{BLIS} without code generation.
The recursive in-place algorithms is similar to the one presented in \cite{bassoy:2019:ttv} and computes the tensor-matrix multiplication by executing \tf{GEMM} with slices and fibers of tensors.
However, the presented algorithm requires twice as many cases and has also additional implementation options which has not been previously discussed.
%that computes the tensor-matrix multiplication by executing \tf{GEMM} with slices and fibers of tensors.
Moreover, except for few corner cases, we demonstrate that our algorithm is able to perform the multiplication with any contraction mode using multiple slice-matrix multiplications and only one \tf{GEMM} parameter configuration.
For parallel execution, we propose a variable loop fusion method with respect to the slice order of slice-vector multiplications. 
Our algorithms support dense tensors with any order, dimensions and any linear tensor layout including the first- and the last-order storage formats for any contraction mode.
We have quantified the impact of the tensor layout, tensor slice order and parallel execution of slice-matrix multiplications with varying contraction modes.
The runtime measurements of our implementations are compared with those presented in \cite{springer:2018:design, abadi:2016:tensorflow,matthews:2018:high}.
In summary, the main findings of our work are:
\begin{itemize}
	\item 
	%
	A tensor-matrix multiplication is implementable by an in-place algorithm with $1$ \tf{GEMV} and $7$ \tf{GEMM} parameter configurations supporting all combinations of contraction mode, tensor order and dimensions.
	%TODO: integrate the recipe from  \cite{dinapoli:2014:towards.efficient.use}
	% validating the second recipe in \cite{dinapoli:2014:towards.efficient.use} with a precise description.
	\item 
	Algorithms with variable loop fusion and parallel slice-matrix multiplications can achieve the peak performance of a \tf{GEMM} with large slice dimensions. %  for about $45$\% of about tests cases with asymmetrically shaped tensors of $30$ (16) Gflops/s \tf{OpenBLAS}'s 
	Moreover, the proposed algorithm is layout oblivious and is able to achieve a sustainable performance throughput for any linear tensor layout.
	%TODO: do you want to test slicing?
	%The use of order-$2$ tensor slices helps to retain the performance at a peak level.
	\item
	A \tf{LOG}-based tensor-times-matrix implementation can be faster than \tf{TTGT}- and \tf{GETT}-based implementations that have been described in \cite{springer:2018:design, matthews:2018:high}.
	%TODO: Put in the numbers.
	Using symmetrically shaped tensors, an average speedup of \tq x to \tq x for single and double precision floating point computations can be achieved.
\end{itemize}

The remainder of the paper is organized as follows. 
Section~\ref{sec:related} presents related work.
Section~\ref{sec:preliminaries} introduces the terminology used in this paper and defines the tensor-vector multiplication.
Algorithm design and methods for parallel execution is discussed in Section~\ref{sec:design}.
Section~\ref{sec:experimental.setup} describes the test setup and discusses the benchmark results in Section \ref{sec:results}.
Conclusions are drawn in Section~\ref{sec:conclusion}.


\begin{comment}
In Introduction these are the claims and benefits:
\begin{itemize}
\item we propose recursive tensor vector multiplication algorithms that can process tensors with any rank, dimension extent and any linear layouts (including the first- and the last-order storage format) for any possible contraction mode (show algorithms).
\item while this might not possible for the general tensor-tensor multiplication, we show that the tensor-vector multiplication algorithms can be defined in terms of tensors and subtensors in terms of four and three general matrix vector multiplication configurations that is applicable for any kind of contraction mode and linear layout combination. This greatly simplifies the implementation of the tensor-vector multiplication. (show tables for tensor and subtensor)
\item We present a parallelization strategy with which the number of parallel executed matrix-vector multiplications is adjusted at runtime. In contrast to a simple parallelization over the most outer loop we show that our proposed strategy leads generally to a sustained bandwith.
\item Using about 2x10x360x120 tensor shape configurations the proposed algorithms provide a sustained throughput of 20 GB/s for any kind of contraction mode and linear layout combination. We show that our methodology simplifies the design and implementation of such a complicated operation without any matrification.
\item We are able to transform a Multi-Loop GEMV into a Single-Loop GEMV which can be fully parallelized with the parallel OpenMP.
\item We show parallelization strategies for both subtensor sizes without employing 
\item The analysis of tensor contraction is hard due to great design space. A tensor is parametrized in terms of order x dimensions x layout. Additionally, the contraction operation itself is highly variable: only for the class 2 operation with no free indices on the right-hand side you have contraction mode x ???. Understanding what blocking strategy is best is hard to find out   give an example from GETT-Paper that 'only' 50 examples of tensor contractions are considered. (by the way how does we have?, can we divide the analysis space into tensor parametrization and algorithm parametrization?)	
\item it would be important to mention that the compiler of Paul Springer also considers the utilization of different approach to generate highly efficient code. just to mention that the search space for the selection of the right approach is great. our work is based on the loop-over-gemm approach.
\item According to \cite{li:2015:input} the transposition for out-of-place tensor contractions can take up to $70$\% of their execution time.
\item Our approach is similar to \cite{li:2015:input} which uses coarse-grained parallelism via OpenMP together with fine-graind parallelism using high-performance \ttt{gemm}. 
Similar to this work it states that the tensor-matrix multiplication can be computed by a sequence of matrix multiplies formed either from the left-most or the right-most contiguous modes. 
We can state the same for the tensor-vector multiplication that any tensor-vector multiplication can be expressed in terms of slice-vector multiplications that are executable in-place by the linear algebra \ttt{gemv} routine without transposition.
It uses a nonlinear recursive approach that has been already applied for elementwise tensor operations in \cite{bassoy:2018:fast}.
Our proposed algorithms implement the mode-q tensor-vector multiplication supporting tensors with runtime or compile-time variable order and dimensions. Tensor Elements can be stored according to any non-hierarchical layout including the first- and last-order storage format. We precisely define the mapping between a tensor-vector multiplication and a \ttt{gemv} for any order, storage layout and contraction mode.
Complying with Lemma 4.1. in \cite{li:2015:input} we show that in case of a tensor-vector multiplication, can be performed without physical reorganization on exactly $p-q$ modes that do not have to be contiguous.
\item we show that our mapping strategy yields is up to 20x faster than \ttt{TTGT}-like and up to 10x faster than \ttt{GETT}-like implementations.
\item Most publications use a generator to construct multi-loops with high-performance kernels.
\end{itemize}
\end{comment}
