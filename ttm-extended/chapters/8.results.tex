\section{Results and Discussion}
\label{sec:results}
\begin{figure*}[t]
\input{figures/performance.tlib.contour-mkl}  %parloop
\input{figures/performance.tlib.contour-mkl-pargemm}  
%\input{figures/performance.tlib.contour-aocl}
\caption{%
\footnotesize %
Performance contour plots in double-precision Gflops per core of the proposed TTM algorithms \tf{<par-loop>} and \tf{<par-gemm>} with varying tensor orders $p$ and contraction modes $q$. 
The top row of maps (1.x) depict measurements of the \tf{<par-loop>} versions while bottom row of maps with number (2.x) contain measurements of the \tf{<par-gemm>} versions.
Tensors are asymmetrically shaped on the left four maps (a,b) and symmetrically shaped on the right four maps (c,d).
Tensor $\mubA$ and $\mubC$ have the first-order while matrix $\mbB$ has the row-major ordering.
All functions have been measured on the Intel Xeon Gold 5318Y processor.
\label{fig:performance.tlib.contour}
}
\end{figure*}


\begin{figure*}[t]
\input{figures/performance.tlib.case8-cdf-mkl}
\input{figures/performance.tlib.case8-cdf-aocl}
\caption{ %
\footnotesize%
Cumulative performance distributions of the proposed algorithms for the eighth case.
Each distribution line belongs to one algorithm:
\tf{<gemm\_batch>} (\ref{coord:gemm_batch}) , %
\tf{<par-gemm>} (\ref{coord:seq_loops_par_gemm_slice}) and
\tf{<par-loop>} (\ref{coord:par_loops_seq_gemm_slice}) using tensor slices,
\tf{<par-gemm>} (\ref{coord:seq_loops_par_gemm_subtensor}) and
\tf{<par-loop>} (\ref{coord:par_loops_seq_gemm_subtensor}) using subtensors.
Tensors are asymmetrically (left plot) and symmetrically shaped (right plot).
}
\label{fig:performance.tlib.case8}
\end{figure*}


\begin{figure*}[t]
\input{figures/performance.tlib.format.mkl}
\input{figures/performance.tlib.format.aocl}
%\input{figures/performance.tlib.format}
\caption{ %
\footnotesize%
Box plots visualizing performance statics in double-precision Tflops of \tf{<gemm\_batch>} (left) and \tf{<par-loop>} with subtensors (right).
Box plot number $k$ denotes the $k$-order tensor layout of symmetrically shaped tensors with order $7$.
% where the $1$-order and $7$-order layouts are the first- and last-order storage formats, respectively.
}
\label{fig:performance.tlib.format}
\end{figure*}

\begin{figure*}[t]
\input{figures/performance.comparison-mkl}
\input{figures/performance.comparison-aocl} 
\caption{ %
\footnotesize%
Cumulative performance distributions of tensor-times-matrix algorithms in double-precision Tflops.
Each distribution line belongs to a library:
\textbf{tlib}[ours] (\ref{coord:nonsymmetric.tlib.slice}), %
\textbf{tcl} (\ref{coord:nonsymmetric.tcl}), %
\textbf{tblis} (\ref{coord:nonsymmetric.tblis}), %
\textbf{libtorch} (\ref{coord:nonsymmetric.libtorch}), %
\textbf{eigen} (\ref{coord:nonsymmetric.eigen}).
Libraries have been tested with asymmetrically-shaped (left plot) and symmetrically-shaped tensors (right plot).
}
\label{fig:performance.comparison}
\end{figure*}


\subsection{Slicing Methods}
This section analyzes the performance of the two proposed slicing methods \ttt{<slice>} and \ttt{<subtensor>} that have been discussed in section \ref{subsec:parallel.multi-loops}.
%Note that this analysis is equal to minimizing or maximizing the input parameter $M_C$.
Figure \ref{fig:performance.tlib.contour} contains eight performance contour plots of four \ttt{ttm} functions \ttt{<par-loop>} and \ttt{<par-gemm>} that either compute the slice-matrix product with subtensors \ttt{<subtensor>} or tensor slices \ttt{<slice>}.
Each contour level within the plots represents an average Gflops/core value, averaged across tensor sizes.

Moreover, each contour plot contains all applicable \ttt{gemm} cases listed in Table \ref{tab:mapping_rm_cm}.
The first column of performance values is generated by \ttt{gemm} belonging to case $3$, except the first element which corresponds to case $2$.
The first row, excluding the first element, is generated by case $6$ function.
Case $7$ is covered by the diagonal line of performance values when $q = p$.  
Although Figure \ref{fig:performance.tlib.contour} suggests that $q>p$ is possible, our profiling program sets $q=p$.
Finally, case $8$ with multiple \ttt{gemm} calls is represented by the triangular region which is defined by $1<q<p$.


% With <par-loop,seq-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-loop>} with \ttt{<slice>} averages with asymmetrically shaped tensors $26.22$ Gflops/core ($1.67$ Tflops).
With a maximum performance of $43.35$ Gflops/core ($2.77$ Tflops), it performs on average $89.64$\% faster than function \ttt{<par-loop>} with subtensors.
The slowdown with subtensors at $q=p-1$ or $q=p-2$ can be explained by the small loop count of the function that are in these cases $2$ and $4$, respectively.
While function \ttt{<par-loop>} with tensor slices is affected by the tensor shapes for dimensions $p=3$ and $p=4$ as well, its performance improves with increasing order due to the increasing loop count.

%Symmetrically Shaped Tensors
Function \ttt{<par-loop>} with tensor slices achieves on average $13.01$ Gflops/core ($832.42$ Gflops) with symmetrically shaped tensors.
In this case, \ttt{<par-loop>} with subtensors achieves a mean throughput of $13.22$ Gflops/core ($846.16$ Gflops) and is on average $9.89$\% faster than the \ttt{<slice>} version.
The performances of both functions are monotonically decreasing with increasing tensor order, see plots (1.c) and (1.d) in Figure \ref{fig:performance.tlib.contour}.
The average performance decrease of both functions can be approximated by a cubic polynomial with the coefficients $-35$, $640$, $-3848$ and $8011$.

% With <seq-loop,par-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-gemm>} with tensor slices averages $27.31$ Gflops/core ($1.74$ Tflops) and achieves up to $43.43$ Gflops/core ($2.77$ Tflops).
With subtensors, function \ttt{<par-gemm>} exhibits almost identical performance characteristics and is on average only $3.42$\% slower than its counterpart with tensor slices.
% Symmetrically Shaped Tensors
For symmetrically shaped tensors, \ttt{<par-gemm>} with subtensors and tensor slices achieve a mean throughput $11.98$ Gflops/core ($767.31$ Gflops) and $11.57$ Gflops/core ($740.67$ Gflops), respectively.
However, function \ttt{<subtensor>} is on average $87.74$\% faster than the \ttt{slice} which is hardly visible due to small performance values around $5$ Gflops/core or less whenever $q<p$ and the dimensions are smaller than $256$.
The speedup of function \ttt{<subtensor>} version can be explained by the smaller loop count and slice-matrix multiplications with larger tensor slices.

%TODO: we could say something about the peak performance and how much is achievable and how much is achieved.
%TODO: we could say something about the perofrmances about the corner cases as well. This could be left to the section after the next.
%TODO: do we need par-gemm and seq-gemm or par-loop and seq-loop, or is it enough do we have par-gemm or par-loop do denote the opposite. I just changed that. rechange if desired.

\subsection{Parallelization Methods}
This section discusses the performance results of the two parallelization methods \ttt{<par-gemm>} and \ttt{<par-loop>} using the same Figure \ref{fig:performance.tlib.contour}.

% Asymmetrically shaped tensors
With asymmetrically shaped tensors, both \ttt{<par-gemm>} functions with subtensors and tensor slices compute the tensor-matrix product with $27$ Gflops/core and outperform function \ttt{<par-loop>}\allowbreak\ttt{<subtensor>} version on average by a factor of $2.31$.
The speedup can be explained by the performance drop of function \ttt{<par-loop>}\allowbreak\ttt{<subtensor>} to $3.49$ Gflops/core at $q=p-1$ while both \ttt{<par-gemm>} functions operate around $39$ Gflops/core.
Function \ttt{<par-loop>} with tensor slices performs better for reasons explained in the previous subsection.
It is on average $30.57$\% slower than its \ttt{<par-gemm>} version due to the aforementioned performance drops.

% Symmetrically shaped tensors
In case of symmetrically shaped tensors, \ttt{<par-loop>} with subtensors and tensor slices outperform their corresponding \ttt{<par-gemm>} counterparts by $23.3$\% and $32.9$\%, respectively.
The speedup mostly occurs when $1<q<p$ where the performance gain is a factor of $2.23$.
This performance behavior can be expected as the tensor slice sizes decreases for the eighth case with increasing tensor order causing the parallel slice-matrix multiplication to perform on smaller matrices.
In contrast, \ttt{<par-loop>} can execute small single-threaded slice-matrix multiplications in parallel.

%TODO: Summarizing: * use par-loop with subtensors on symmetrically shaped tensors where the leading and contraction dimensions are relatively small. * use par-gemm with tensor slices or subtensors if your leading and contraction dimensions are large compared to the other dimensions.



%TODO: Case 2,3 and Case 6,7 are about the same.
%TODO: Most differences at Case 8

\subsection{Case 8 with different Matrix Formats and Proessors}
The contour plots in Fig. \ref{performance.tlib.contour} contain performance data of all cases except for $4$ and $5$, see Table \ref{tab:mapping}.
The effects of the presented slicing and parallelization methods can be better understood if performance data of only the eighth case is examined.
Fig. \ref{fig:performance.tlib.case8} contains cumulative performance distributions of all the proposed algorithms which are generated \tf{gemm} or \tf{gemm\_batch} calls within case 8.
As the distribution is empirically computed, the probability $y$ of a point $(x,y)$ on a distribution function corresponds to the number of test cases of a particular algorithm that achieves $x$ or less Tflops.
For instance, function \tf{<seq-loops,par-gemm>} with subtensors computes the tensor-matrix product for $50$\% percent of the test cases with equal to or less than 0.6 Tflops in case of asymmetrically shaped tensor.
Consequently, distribution functions with an exponential growth are favorable while logarithmic behavior is less desirable.
The test set cardinality for case 8 is $255$ for asymmetrically shaped tensors and $91$ for symmetrically ones.

In case of asymmetrically shaped tensors, \tf{<par-loops,seq-gemm>} with tensor slices performs best and outperforms \tf{<gemm\_batch>}.
One unexpected finding is that function \tf{<seq-loops,par-gemm>} with any slicing strategy performs better than \tf{<gemm\_batch>} when the tensor order $p$ and contraction mode $q$ satisfy $4 \leq p \leq 7$ and $2 \leq q \leq 4$, respectively.
Functions executed with symmetrically shaped tensors reach at most $743$ Gflops for the eighth case which is less than half of the attainable peak performance of $1.7$ Tflops.
This is expected as cases $2$ and $3$ are not considered.
%for $p=2$ with $q=1$ and $q=2$ 
Functions \tf{<par-loops,seq-gemm>} with subtensors and \tf{<gemm\_batch>} have almost the same performance distribution outperforming \tf{<seq-loops,par-gemm>} for almost every test case.
%We did not show the performance results of \tf{<par-loops,par-gemm>} as they are almost identical to the ones of \tf{<par-loops,seq-gemm>}, independent of the slicing method and tensor shape.
Function \tf{<par-loops,seq-gemm>} with tensor slices is on average almost as fast as with subtensors.
However, if the tensor order is greater than $3$ and the tensor dimensions are less than $64$, its running time increases by almost a factor of $2$.

These observations suggest to use \tf{<par-loops,seq-gemm>} with tensor slices for common cases in which the leading and contraction dimensions are larger than $64$ elements.
Subtensors should only be used if the leading dimension $n_{\pi_1}$ of $\mubA_{\pi_1,q}$ and $\mubC_{\pi_1,q}$ falls below $64$.
This strategy is different to the one presented in \cite{li:2015:input} that maximizes the number of modes involved in the matrix multiply.
We have also observed no performance improvement if \tf{par-gemm} was used with \tf{par-loops} which is why their distribution functions are not shown in Fig. \ref{fig:performance.tlib.case8}.
Moreover, in most cases the \tf{seq-loops} implementations are independent of the tensor shape slower than \tf{par-loops}, even for smaller tensor slices.
\vspace{-1em}

\subsection{Layout-Oblivious Algorithms}
Fig. \ref{fig:performance.tlib.format} contains two subfigures visualizing performance statics in double-precision Tflops of \tf{<gemm\_batch>} (left subfigure) and \tf{<par-loops,seq-gemm>} with subtensors (right subfigure).
Each box plot with the number $k$ has been computed from benchmark data with symmetrically shaped order-$7$ tensors with the $k$-order tensor layout.
The $1$-order and $7$-order layout, for instance, are the first- and last-order storage formats for the order-$7$ tensor with $\pi_F=(1,2,...,7)$ and $\pi_L= (7,6,...,1)$.
The definition of $k$-order tensor layouts can be found in section \ref{sec:preliminaries:layout}.

The low performance of around $70$ Gflops can be attributed to the fact that the contraction dimension of subtensors of tensor slices of symmetrically shaped order-7 tensors are $8$ while the leading dimension is $8$ or at most $48$ for subtensors.
The relative standard deviation of \tf{<gemm\_batch>}'s and \tf{<par-loops,seq-gemm>}'s median values are $12.95$\% and $17.61$\%.
Their respective interquartile range are similar with a relative standard deviation of $22.25$\% and $15.23$\%.

The runtime results with different $k$-order tensor layouts show that the performance of our proposed algorithms is not designed for a specific tensor layout.
Moreover, the performance stays within an acceptable range independent of the tensor layout.
\vspace{-1em}

\subsection{Comparison with other Approaches}
We have compared the best performing algorithm with four libraries that implement the tensor-matrix multiplication.

Library \tbf{tcl} implements the TTGT approach with a high-perform tensor-transpose library \tbf{hptt} which is discussed in \cite{springer:2018:design}.
\tbf{tblis} implements the GETT approach that is akin to Blis' algorithm design for the matrix multiplication \cite{matthews:2018:high}.
The tensor extension of \tbf{eigen} (v3.3.7) is used by the Tensorflow framework.
Library \tbf{libtorch} (v2.3.0) is the \tf{C++} distribution of PyTorch.
\tbf{tlib} denotes our library using algorithm \tf{<par-loops,seq-gemm>} that have been presented in the previous paragraphs.

Fig. \ref{fig:performance.tlib.case8} contains cumulative performance distributions for the complete test sets comparing the performance distribution of our implementation with the previously mentioned libraries.
Note that we only have used tensor slices for asymmetrically shaped tensors (left plot) and subtensors for symmetrically shaped tensors (right plot).
Our implementation with a median performance of $793.75$ Gflops outperforms others' for almost every asymmetrically shaped tensor in the test set.
The median performances of tcl, tblis, libtorch and eigen are $503.61$, $415.33$, $496.22$ and $244.69$ Gflops reaching on average $74.11$\%, $61.14$\%, $76.68$\% and $39.34$\% of tlib's throughputs.

In case of symmetrically shaped tensors the performance distributions of all libraries on the right plot in Fig. \ref{fig:performance.tlib.case8} are much closer.
The median performances of tlib, tblis, libtorch and eigen are $228.93$, $208.69$, $76.46$, $46.25$ Gflops reaching on average $73.06$\%, $38.89$\%, $19.79$\% of tlib's throughputs\footnote{We were unable to run tcl with our test set containing symmetrically shaped tensors. We suspect a very high memory demand to be the reason.}.
All libraries operate with $801.68$ or less Gflops for the cases 2 and 3 which is almost half of tlib's performance with $1579$ Gflops.
The median performance and the interquartile range of tblis and tlib for the cases 6 and 7 are almost the same.
Their respective median Gflops are $255.23$ and $263.94$ for the sixth case and $121.17$ and $144.27$ for the seventh case.
This explains the similar performance distributions when their performance is less than $400$ Gflops.
Libtorch and eigen compute the tensor-matrix product, in median, with $17.11$ and $9.64$ Gfops/s, respectively.
Our library tlib has a median performance of $102.11$ Gflops and outperforms tblis with $79.35$ Gflops for the eighth case.


%Comparing \tf{TCL} performance, \tf{TLIB-SB-PN} achieves an average speedup of $6$x and more than $8$x for $42$\% of the test cases with asymmetrically shaped tensors and executes on average $5$x faster with symmetrically shaped tensors.
%In comparison with \tf{TBLIS}, \tf{TLIB-SB-PN} computes the tensor-vector product on average $4$x and $3.5$x faster for asymmetrically and symmetrically shaped tensors, respectively.