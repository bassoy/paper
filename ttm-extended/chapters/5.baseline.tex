\section{Algorithm Design}
\label{sec:design}
\subsection{Baseline Algorithm with Contiguous Memory Access}
\label{sec:design:modified.baseline.algorithm}
The tensor-times-matrix multiplication in equation \ref{equ:tensor.matrix.multplication} can be implemented with one sequential algorithm using a nested recursion \cite{bassoy:2018:fast}.
It consists of two \ttt{if} statements with an \ttt{else} branch that computes a fiber-matrix product with two loops.
The outer loop iterates over the dimension $m$ of $\mubC$ and $\mbB$, while the inner iterates over dimension $n_q$ of $\mubA$ and $\mbB$ computing an inner product with fibers of $\mubA$ and $\mbB$. 
While matrix $\mbB$ can be accessed contiguously depending on its storage format, elements of $\mubA$ and $\mubC$ are accessed non-contiguously if $\pi_1 \neq q$.

%The access pattern can be improved by reordering tensor elements according to the storage format.
%However, copy operations reduce the overall throughput of the operation, see \cite{shi:2016:tensor.contraction}.


A better approach is illustrated in algorithm \ref{alg:ttm.sequential.coalesced} where the loop order is adjusted to the tensor layout $\mbpi$ and memory is accessed contiguously for $\pi_1 \neq q$ and $p > 1$.
The adjustment of the loop order is accomplished in line 5 which uses the layout tuple $\mbpi$ to select a multi-index element $i_{\pi_r}$ and to increment it with the corresponding stride $w_{\pi_r}$.
Hence, with increasing recursion level and decreasing $r$, indices are incremented with smaller strides as $w_{\pi_r} \leq w_{\pi_{r+1}}$.
%The condition of the second \ttt{if} statement in line 4 is changed from $r \geq 1$ to $r > 1$.
The second \tf{if} statement in line number 4 allows the loop over mode $\pi_1$ to be placed into the base case which contains three loops performing a slice-matrix multiplication.
%the minimum stride $w_{\pi_1}$ to be included in the base case.
%The latter now contains three loops performing a slice-matrix multiplication. 
%The loop ordering are adjusted according to the tensor and matrix layout.
In this way, the inner-most loop is able to increment $i_{\pi_1}$ with a unit stride and contiguously accesses tensor elements of $\mubA$ and $\mubC$.
The second loop increments $i_q$ with which elements of $\mbB$ are contiguously accessed if $\mbB$ is stored in the row-major format.
The third loop increments $j$ and could be placed as the second loop if $\mbB$ is stored in the column-major format.
%The loop ordering are adjusted according to the tensor and matrix layout.
%The simple ordering of the three loops is discussed in \cite{golub:2013:matrix.computations}.

\begin{algorithm}[t]
%\SetAlgoNoLine
\DontPrintSemicolon
\SetKwProg{Fn}{}{}{end}
\SetKwFunction{function}{ttm}%
%\SetAlgoNoEnd
\footnotesize 
\SetAlgoVlined
\hrule
\BlankLine
\Fn{\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r$}}
{
	\uIf{$r = \mhq$ }
	{
		\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$ }
	}
	\uElseIf{$r > 1$ }
	{
		\For{$i_{\pi_r} \leftarrow 1$ \KwTo $ n_{\pi_r}$}
		{
			\function{$\mubA, \mbB, \mubC, \mbn, \mbi, m, q, \mhq, r-1$}\;
		}		
	}	
	\Else%{$r\geq1 \wedge m \neq 1$}
	{
		\For{$j \leftarrow 1$ \KwTo $m$}
		{
			\For{$i_q \leftarrow 1$ \KwTo $n_q$}
			{			
				\For{$i_{\pi_1} \leftarrow 1$ \KwTo $n_{\pi_1}$}
				{
					$\mubC([\mbi_1,j,\mbi_2])$ \ttt{+=} $\mubA([\mbi_1,i_q,\mbi_2]) \cdot \mbB(j,i_q)$\;
				}
			}
		}
	}
}
\BlankLine
\hrule
\caption{
\footnotesize %
Modified baseline algorithm with contiguous memory access for the tensor-matrix multiplication.
The tensor order $p$ must be greater than $1$ and the contraction mode $q$ must satisfy $1 \leq q \leq p$ and $\pi_1 \neq q$.
The initial call must happen with $r=p$ where $\mbn$ is the shape tuple of $\mubA$ and $m$ is the $q$-th dimension of $\mubC$. 
%Iteration along mode $q$ with $\mhq = \mbpi^{-1}_q$ is moved into the inner-most recursion level.
\label{alg:ttm.sequential.coalesced}
}
\end{algorithm}

While spatial data locality is improved by adjusting the loop ordering, slices $\mubA_{\pi_1,q}'$, fibers $\mubC_{\pi_1}'$ and elements $\mubB(j,i_q)$ are accessed $m$, $n_q$ and $n_{\pi_1}$ times, respectively.
The specified fiber of $\mubC$ might fit into first or second level cache, slice elements of $\mubA$ are unlikely to fit in the local caches if the slice size $n_{\pi_1} \times n_q$ is large, leading to higher cache misses and suboptimal performance.
Instead of optimizing for better temporal data locality, we use existing high-performance BLAS implementations for the base case.
The following subsection explains this approach.

\subsection{BLAS-based Algorithms with Tensor Slices}
\label{sec:design:blas.based.algorithm.slices}
%\vspace{-0.3em}
\begin{table*}[t]
%\captionsetup{width=0.7\textheight}
\centering
\footnotesize
%\scriptsize
\begin{tabular}{ c c c c c c c c c c c c c c c } % 
\toprule
Case & Order $p$ & Layout $\mbpi_{\mubA,\mubC}$ & Layout $\mbpi_{\mbB}$ & Mode $q$ & Routine & \tf{T} & \tf{M} & \tf{N} & \tf{K} & \tf{A} & \tf{LDA} & \tf{B} & \tf{LDB} & \tf{LDC} \\
\midrule
1 & $1$ & -       & \tf{rm/cm} & $1$   & \tf{gemv} & -       & $m$   & $n_1$ & -     & $\mbB$  & $n_1$ & $\mubA$  & - & - \\
\midrule
2 & $2$ & \tf{cm} & \tf{rm} & $1$      & \tf{gemm} & $\mbB$  & $n_2$ & $m$   & $n_1$ & $\mubA$ & $n_1$ & $\mbB$   & $n_1$ & $m$   \\
  & $2$ & \tf{cm} & \tf{cm} & $1$      & \tf{gemm} & -       & $m$   & $n_2$ & $n_1$ & $\mbB$  & $m$   & $\mubA$  & $n_1$ & $m$   \\
3 & $2$ & \tf{cm} & \tf{rm} & $2$      & \tf{gemm} & -       & $m$   & $n_1$ & $n_2$ & $\mbB$  & $n_2$ & $\mubA$  & $n_1$ & $n_1$ \\
  & $2$ & \tf{cm} & \tf{cm} & $2$      & \tf{gemm} & $\mbB$  & $n_1$ & $m$   & $n_2$ & $\mubA$ & $n_1$ & $\mbB$   & $m$   & $n_1$ \\  
4 & $2$ & \tf{rm} & \tf{rm} & $1$      & \tf{gemm} & -       & $m$   & $n_2$ & $n_1$ & $\mbB$  & $n_1$ & $\mubA$  & $n_2$ & $n_2$ \\
  & $2$ & \tf{rm} & \tf{cm} & $1$      & \tf{gemm} & $\mbB$  & $n_2$ & $m$   & $n_1$ & $\mubA$ & $n_2$ & $\mbB$   & $m$   & $n_2$ \\
5 & $2$ & \tf{rm} & \tf{rm} & $2$      & \tf{gemm} & $\mbB$  & $n_1$ & $m$   & $n_2$ & $\mubA$ & $n_2$ & $\mbB$   & $n_2$ & $m$   \\
  & $2$ & \tf{rm} & \tf{cm} & $2$      & \tf{gemm} & -       & $m$   & $n_1$ & $n_2$ & $\mbB$  & $m$   & $\mubA$  & $n_2$ & $m$   \\
\midrule
6 & $>2$ & any    & \tf{rm} & $\pi_1$  & \tf{gemm} & $\mbB$  & $\mbnq$ & $m$     & $n_q$ & $\mubA$ & $n_q$   & $\mbB$  & $n_q$   & $m$\\
  & $>2$ & any    & \tf{cm} & $\pi_1$  & \tf{gemm} & -       & $m$     & $\mbnq$ & $n_q$ & $\mbB$  & $m$     & $\mubA$ & $n_q$   & $m$\\
7 & $>2$ & any    & \tf{rm} & $\pi_p$  & \tf{gemm} & -       & $m$     & $\mbnq$ & $n_q$ & $\mbB$  & $n_q$   & $\mubA$ & $\mbnq$ & $\mbnq$ \\
  & $>2$ & any    & \tf{cm} & $\pi_p$  & \tf{gemm} & $\mbB$  & $\mbnq$ & $m$     & $n_q$ & $\mubA$ & $\mbnq$ & $\mbB$  & $m$     & $\mbnq$ \\
\midrule
8 & $>2$ & any    & \tf{rm} & $\pi_2,..,\pi_{p-1}$ & \tf{gemm*} & -      & $m$ & $n_{\pi_1}$ & $n_q$ & $\mbB$  & $n_q$ & $\mubA$ & $w_q$ & $w_q$ \\
  & $>2$ & any    & \tf{cm} & $\pi_2,..,\pi_{p-1}$ & \tf{gemm*} & $\mbB$ & $n_{\pi_1}$ & $m$ & $n_q$ & $\mubA$ & $w_q$ & $\mbB$  & $m$   & $w_q$ \\
\bottomrule
\end{tabular}
%\vspace{0.2cm}
\caption%
{%
\footnotesize
Eight cases of CBLAS routines for the mode-$q$ tensor-matrix multiplication.
%Argument for \tf{gemv} and \tf{gemm} with 
Arguments \tf{T}, \tf{M}, \tf{N}, etc. of \tf{gemv} and \tf{gemm} are chosen with respect to the tensor order $p$, layout $\mbpi$ of $\mubA$, $\mbB$, $\mubC$ and contraction mode $q$ where \tf{T} specifies if $\mbB$ is transposed.
Routine \tf{gemm*} with a star denotes multiple \tf{gemm} calls with different tensor slices.
Argument $\bar{n}_q$ for case 6 and 7 is defined as $\bar{n}_q = (\prod_r^p n_r)/n_q$.
The input matrix $\mbB$ is either stored in column-major \tss{cm} or row-major \tss{rm} order.
The element ordering of the tensor slices is determined by the storage format of the input matrix $\mbB$.
}
\label{tab:mapping_rm_cm}
\end{table*}
Algorithm \ref{alg:ttm.sequential.coalesced} computes the mode-$q$ tensor-matrix product in a recursive fashion.
The base case multiplies different tensor slices of $\mubA$ with the matrix $\mbB$.
Instead of optimizing the slice-matrix multiplication in the base case, one can use a \ttt{gemm} routine instead.
Note that algorithm \ref{alg:ttm.sequential.coalesced} is the general case for $p\geq 2$ but only executable for $\pi_1 \neq q$.
For $\pi_1 = q$, the tensor-matrix product can be computed by a matrix-matrix multiplication where the input tensor $\mubA$ can be flattened into a matrix without any copy operation.
The same can be applied when $\pi_p = q$ and five other cases where the input tensor is either one or two-dimensional.
In summary, there are seven other corner cases to the general case where a single \tf{gemv} or \ttt{gemm} call suffices to compute the tensor-matrix product.
All eight cases per storage format are listed in table \ref{tab:mapping_rm_cm}.
The arguments of the CBLAS routines \tf{gemv} or \ttt{gemm} are set according to the tensor order $p$, tensor layout $\mbpi$ and contraction mode $q$.
If the input matrix $\mbB$ has the row-major order, parameter \tf{CBLAS\_ORDER} of function \ttt{gemm} is set to \tf{CblasRowMajor} and \tf{CblasColMajor} otherwise.
Note that table \ref{tab:mapping_rm_cm} supports all linear tensor layouts of $\mubA$ and $\mubC$ with no limitations on tensor order and contraction mode.
The following subsection describes all eight cases when the input matrix $\mbB$ has the row-major order.

%Note , all linear tensor layouts can be supported by setting the remaining parameters of \ttt{gemm}.

%TODO: previously explain how CBLAS is called and what options one has, i.e. explain the design space
%TODO: explain the table for B format (already explained for RM) and CBLAS multiplication (already explained for RM) as on subsubsection and include two other subsection explaining what changes if we take B.format CM and another subsection explaining how the parameters change if it is CM.

%We apply highly optimized routines to fully or partly execute tensor contractions as it is done in \cite{li:2015:input, shi:2016:tensor.contraction}.
%The function and parameter configurations for the tensor multiplication can be divided into eight cases.

%Table \ref{tab:mapping} extends the finding in \cite{dinapoli:2014:towards.efficient.use} precisely defining the mapping for any storage format. 
%It also complies with the findings in \cite{li:2015:input}.
\subsubsection{Row-Major Matrix Multiplication}

\tit{Case 1:}
If $p=1$, The tensor-vector product $\mubA \times_1 \mbB$ can be computed with a \tf{gemv} operation where $\mubA$ is an order-$1$ tensor $\mba$ of length $n_1$ such that $\mba^T \cdot \mbB$.

\tit{Case 2-5:}
If $p=2$, $\mubA$ and $\mubC$ are order-$2$ tensors with dimensions $n_1$ and $n_2$.
In this case the tensor-matrix product can be computed with a single \ttt{gemm}.
If $\mbA$ and $\mbC$ have the column-major format with $\mbpi=(1,2)$, \ttt{gemm} either executes $\mbC = \mbA \cdot \mbB^T$ for $q =1$ or $\mbC = \mbB \cdot \mbA$ for $q=2$.
Reshaping both matrices using $\rho$ with $\mbrho = (2,1)$, \ttt{gemm} interprets $\mbC$ and $\mbA$ as matrices in row-major format although both are stored column-wise.
If $\mbA$ and $\mbC$ have the row-major format with $\mbpi=(2,1)$, \ttt{gemm} either executes $\mbC = \mbB \cdot \mbA$ for $q =1$ or $\mbC = \mbA \cdot \mbB^T$ for $q=2$. 
The transposition of $\mbB$ is necessary for the cases 2 and 5 which is independent of the chosen layout.

\tit{Case 6-7 :}
% If the order of $\mubA$ and $\mubC$ is greater than $2$ 
%the contraction mode $q$ is equal to $\pi_1$ 
If $p>2$ and if $q=\pi_1$(case 6), a single \ttt{gemm} with the corresponding arguments executes $\mbC = \mbA \cdot \mbB^T$ and computes a tensor-matrix product $\mubC = \mubA \times_{\pi_1} \mbB$.
% for any storage layout of $\mubA$ and $\mubC$
Tensors $\mubA$ and $\mubC$ are flattened with $\varphi_{2,p}$ to row-major matrices $\mbA$ and $\mbC$.
%$f_{2,p}$, see subsection \ref{sec:preliminaries:flattening}.
Matrix $\mbA$ has $\bar{n}_{\pi_1} = \bar{n} / n_{\pi_1}$ rows and $n_{\pi_1}$ columns while matrix $\mbC$ has the same number of rows and $m$ columns.
If $\pi_p=q$ (case 7), $\mubA$ and $\mubC$ are flattened with $\varphi_{1,p-1}$ to column-major matrices $\mbA$ and $\mbC$.
Matrix $\mbA$ has $n_{\pi_p}$ rows and $\bar{n}_{\pi_p} =  \bar{n} / n_{\pi_p}$ columns while $\mbC$ has $m$ rows and the same number of columns.
In this case, a single \ttt{gemm} executes $\mbC = \mbB \cdot \mbA$ and computes $\mubC = \mubA \times_{\pi_p} \mbB$.
Noticeably, the desired contraction are performed without copy operations, see subsection \ref{sec:preliminaries:flattening.reshaping}. 

\tit{Case 8 $(p>2)$:}
If the tensor order is greater than $2$ with $\pi_1\neq q$ and $\pi_p \neq q$, the modified baseline algorithm \ref{alg:ttm.sequential.coalesced} is used to successively call $\bar{n} / (n_q \cdot n_{\pi_1})$ times \ttt{gemm} with different tensor slices of $\mubC$ and $\mubA$.
Each \ttt{gemm} computes one slice $\mubC_{\pi_1,q}'$ of the tensor-matrix product $\mubC$ using the corresponding tensor slices $\mubA_{\pi_1,q}'$ and the matrix $\mbB$.
The matrix-matrix product $\mbC = \mbB \cdot \mbA$ is performed by interpreting both tensor slices as row-major matrices $\mbA$ and $\mbC$ which have the dimensions $(n_q,n_{\pi_1})$ and $(m,n_{\pi_1})$, respectively.
%Please note that Algorithm 2 in \cite{li:2015:input} suggests to transpose matrix $\mbB$.

\subsubsection{Column-Major Matrix Multiplication}
The tensor-matrix multiplication is performed with the column-major version of \ttt{gemm} when the input matrix $\mbB$ is stored in column-major order.
Although the number of \ttt{gemm} cases remains the same, the \ttt{gemm} arguments must be rearranged.
The argument arrangement for the column-major version can be derived from the row-major version by swapping the BLAS arguments of \ttt{M} and \ttt{N}, as well as \ttt{A} and \ttt{B}.
Additionally, the transposition flag for matrix $\mbB$ is toggled and the leading dimension argument of \ttt{A} is swapped to \ttt{LDB} or \ttt{LDA} as well.
The only new argument is the new leading dimension of \ttt{B}.
Given case 4 with the row-major matrix multiplication in table \ref{tab:mapping_rm_cm} where tensor $\mubA$ and matrix $\mbB$ are passed to \ttt{B} and \ttt{A}.
The corresponding column-major version is attained when tensor $\mubA$ and matrix $\mbB$ are passed to \ttt{A} and \ttt{B} where the transpose flag for $\mbB$ is set and the remaining dimensions are adjusted accordingly.


\subsubsection{Matrix Multiplication Variations}
The column-major and row-major versions of \ttt{gemm} can be used interchangeably by adapting the storage format. 
This means that a \ttt{gemm} operation for column-major matrices can compute the same matrix product as one for row-major matrices, provided the arguments are rearranged accordingly.
While the argument rearrangement is similar to the previously outlined one, the arguments associated with the matrices \ttt{A} and \ttt{B} are interchanged.
Specifically, \ttt{LDA} and \ttt{LDB} as well as \ttt{M} and \ttt{N} are swapped, along with the corresponding matrix pointers.
In addtion, the transposition flag must be set for \ttt{A} or \ttt{B} in the new format if \ttt{B} or \ttt{A} is transposed in the original version.

Given case 4 with the column-major matrix multiplication in table \ref{tab:mapping_rm_cm} where tensor $\mubA$ and matrix $\mbB$ are set for \ttt{A} and \ttt{B} and $\mbB$ is transposed.
Then the arguments of an equivalent row-major multiplication for \ttt{A}, \ttt{B}, \ttt{M}, \ttt{N}, \ttt{LDA}, \ttt{LDB} and \ttt{T} are $\mbB$, $\mubA$, $m$, $n_2$, $m$, $n_2$ and $\mbB$.
Another possible matrix multiplication variant that yields the same product is computed when, instead of $\mbB$, both tensors $\mubA$ and $\mubC$ are transposed and their corresponding arguments are adjusted accordingly. 
However, we assume that such reformulations of the matrix multiplication do not outperform the variants shown in Table \ref{tab:mapping_rm_cm}, as we expect the use of highly optimized BLAS libraries.


\subsection{BLAS-Based Algorithms with Subtensors}
\label{sec:design:blas.based.algorithm.subtensors}
The eighth case can be further optimized by slicing larger subtensors and use additional dimensions for the slice-matrix multiplication.
% by utilizing larger subtensors than tensor slices
%This is accomplished 
The selected dimensions must adhere to flatten the subtensor into a matrix without reordering or copying elements, see lemma 4.1 in \cite{li:2015:input}.
%We will use our flattening operation which does not copy or reorder elements, see section \ref{sec:preliminaries:flattening.reshaping}.
% see section \ref{sec:preliminaries:flattening.reshaping}.
The number of additional modes is $\mhq-1$ with $\mhq = \mbpi^{-1}(q)$ and the corresponding modes are $\pi_1,\pi_2,\dots,\pi_{\mhq-1}$.
%with dimensions $n_{\pi_1},\dots,n_{\pi_{\mhq-1}}, n_{q}$
Applying flattening $\varphi_{1,\mhq-1}$ and reshaping $\rho$ with $\mbrho = (2,1)$ on a subtensor of $\mubA$ yields a row-major matrix $\mbA$ with shape $(n_q,\prod_{r=1}^{\mhq-1} n_{\pi_r})$.
Analogously, tensor $\mubC$ becomes a row-major matrix with the shape $(m, \prod_{r=1}^{\mhq-1} n_{\pi_r})$.
This description supports all linear tensor layouts and generalizes lemma 4.2 in \cite{li:2015:input}.

Algorithm \ref{alg:ttm.sequential.coalesced} needs a minor modification so that \ttt{gemm} can be used with flattened subtensors instead of tensor slices.
The non-base case of the modified algorithm only iterates over dimensions with indices that are larger than $\mhq$, omitting the first $\mhq$ modes $\mbpi_{1,\mhq} = (\pi_1, \dots, \pi_{\mhq})$ with $\pi_{\mhq} = q$.
The conditions in line 2 and 4 are changed to $1 < r \leq \mhq$ and $\mhq < r$, respectively.
The single indices of the subtensors $\mubA_{\mbpi_{1,\mhq}}'$ and $\mubC_{\mbpi_{1,\mhq}}'$ are given by the loop induction variables that belong to the $\pi_r$-th loop with $\mhq+1 \leq r \leq p$.  
 
\subsection{Parallel BLAS-based Algorithms}
\label{subsec:parallel.multi-loops}
Next, three parallel approaches for the eighth case.
Note that cases 1 to 7 already call a multi-threaded \ttt{gemm}.
%\vspace{-1em}

\subsubsection{Sequential Loops and Parallel Matrix Multiplication}
A simple approach is to not modify algorithm \ref{alg:ttm.sequential.coalesced} and sequentially call a multi-threaded \ttt{gemm} in the base case as described in subsection \ref{sec:design:blas.based.algorithm.slices}.
This is beneficial if $q = \pi_{p-1}$, the inner dimensions $n_{\pi_1},\dots,n_{q}$ are large or if the outer-most dimension $n_{\pi_{p}}$ is smaller than the available processor cores.
However, when the above conditions are not met, the algorithm executes multi-threaded \ttt{gemm} with small subtensors.
This might lead to a low utilization of available computational resources.
This algorithm version will be referred to as \tf{<seq-loops,par-gemm>}.
% that is executable with subtensors or tensor slices
%\vspace{-1em}

\subsubsection{Parallel Loops and Multithreaded Matrix Multiplication}
A more advanced version of the above algorithm executes a single-threaded \ttt{gemm} in parallel with all available (free) modes.
The number of free modes depends on the tensor slicing.
If subtensors are used, all $\pi_{\mhq+1}, \dots, \pi_{p}$ modes are free and can be used for parallel execution.
In case of tensor slices, only dimensions with indices $\pi_1$ and $\pi_{\mhq}$ are free.
%The corresponding maximum degree of parallelism for both cases is $\prod_{r=\mhq+1}^{p} n_{\pi_r}$ and $\prod_{r=1}^{p} n_{r} / (n_{\pi_1} n_{\pi_{\mhq}})$, respectively.

%whereas 
%todo: Note that the number of free modes depends on the tensor order $p$ and contraction mode $q$.
%\footnote{In \cite{li:2015:input}, free modes are called loop modes and are elements of the set $M_L$.}.

%The parallel execution of free loops is accomplished with OpenMP directives and by flattening (collapsing) all loops within the tree-recursion into one or two loops depending on the available fusible loops.

Using tensor slices for the multiplication, $\mubA$ and $\mubC$ are flattened twice with $\varphi_{\pi_{\mhq+1},\pi_p}$ and $\varphi_{\pi_{2},\pi_{\mhq-1}}$.
The flattened tensors are of order $4$ with dimensions $n_{\pi_1}$, $\mhn_{\pi_2}$, $n_{q}$ or $m$, $\mhn_{\pi_4}$ where $\mhn_{\pi_2} = \prod_{r=2}^{\mhq-1} n_{\pi_r}$ and $\mhn_{\pi_4} = \prod_{r=\mhq+1}^{p} n_{\pi_r}$.
This approach transforms the tree-recursion into two loops.
The outer loop iterates over $\mhn_{\pi_4}$ while the inner loop iterates over $\mhn_{\pi_2}$ calling \ttt{gemm} with slices $\mubA_{\pi_1,q}'$ and $\mubC_{\pi_1,q}'$.
%Hence, only two contraction modes $\pi_1$ and $\pi_q$ are involved in the matrix multiplication.
%\footnote{In \cite{li:2015:input}, contraction modes are component modes and are elements of the set $M_C$.}.
Both loops are parallelized using \tf{omp parallel for} together with the \tf{collapse(2)} and the \tf{num\_threads} clause which specifies the thread number.


If subtensors are used, both tensors are flattened twice with $\varphi_{\pi_{\mhq+1},\pi_p}$ and $\varphi_{\pi_{1},\pi_{\mhq-1}}$. 
The flattened tensors are of order $3$ with dimensions $\mhn_{\pi_1}$, $n_{q}$ or $m$, $\mhn_{\pi_4}$ where $\mhn_{\pi_1} = \prod_{r=1}^{\mhq-1} n_{\pi_r}$ and $\mhn_{\pi_4} = \prod_{r=\mhq+1}^{p} n_{\pi_r}$.
The corresponding algorithm consists of one loops which iterates over $\mhn_{\pi_4}$ calling single-threaded \ttt{gemm} with multiple subtensors $\mubA_{\mbpi',q}'$ and $\mubC_{\mbpi',q}'$ with $\mbpi' = (\pi_1,\dots,\pi_{\mhq-1})$.

Both algorithm variants will be referred to as \tf{<par-loops,seq-gemm>} which can be used with subtensors or tensor slices.
Note that \tf{<seq-loops,par-gemm>} and \tf{<par-loops,seq-gemm>} are opposing versions where either \ttt{gemm} or the free loops are performed in parallel.
The all-parallel version \tf{<par-loops,par-gemm>} executes available loops in parallel where each loop thread executes a multi-threaded \ttt{gemm} with either subtensors or tensor slices.
%\vspace{-1em}

\subsubsection{Multithreaded Batched Matrix Multiplication}
The next version of the base algorithm is a modified version of the general subtensor-matrix approach that calls a single batched \ttt{gemm} for the eighth case.
The subtensor dimensions and remaining \ttt{gemm} arguments remain the same.
The library implementation is responsible how subtensor-matrix multiplications are executed and if subtensors are further divided into smaller subtensors or tensor slices.
This version will be referred to as the \tf{<gemm\_batch>} variant.
